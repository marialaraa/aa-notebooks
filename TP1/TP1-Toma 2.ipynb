{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trabajo Práctico 1 \n",
    "### Clasificación sobre datos simulados. \n",
    "\n",
    "## Introducción\n",
    "Para este trabajo, hemos creado una función generadora de minions. Sobre cada minion, hemos medido 200 características que representan habilidades que poseen en distintas tareas (relacionadas al Mal).  \n",
    "\n",
    "El doctor Nefario ha ideado una fórmula para determinar si un minion es o no apto para concretar su plan para conquistar el mundo. De esta manera ha etiquetado más de 500 minions. Lamentablemente, ha perdido dicha fórmula y necesita seguir decidiendo si nuevos minions son o no aptos para su macabro plan.\n",
    "\n",
    "Es por esto que nuestro objetivo será construir clasificadores que estimen lo mejor posible la probabilidad de que nuevos minions sean o no aptos para concretar el plan de conquista y así facilitarle las cosas al doctor Nefario.\n",
    "\n",
    "Por otra parte, ya que el doctor Nefario tuvo problemas con equipos que sobreestiman sus resultados, decidió guardarse varias etiquetas extra que no compartirá con nadie, y que luego utilizará para elegir al mejor equipo, al cual contratará para (de una vez por todas) conquistar el mundo. \n",
    "\n",
    "\n",
    "En concreto:\n",
    "\n",
    "Tendrán disponible una matriz de datos $X$ de $500$ filas en donde cada fila $x^{(i)}$ representa un vector de $200$ características de cada instancia. Es decir, $\\textbf{x}^{(i)} = x_1^{(i)}, \\dots, x_{200}^{(i)}$ con $i$ entre $1$ y $500$. Además, tendrán y, un vector de $500$ posiciones con dos posibles valores: $True$ y $False$. \n",
    "\n",
    "Por otra parte, tendrán disponibles más instancias de evaluación $X_{competencia}$ sin las respectivas etiquetas que utilizaremos para evaluar sus resultados. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brunobian/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n",
      "/home/brunobian/anaconda3/lib/python3.7/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "# PREAMBULOS\n",
    "%matplotlib inline\n",
    "\n",
    "SEED = 1234\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "from IPython.display import display, HTML\n",
    "from time import time\n",
    "\n",
    "import numpy as np\n",
    "np.set_printoptions(precision=4)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "import pandas as  pd\n",
    "pd.set_option('display.max_rows', 10)\n",
    "pd.set_option('display.max_columns', 15)\n",
    "\n",
    "pd.set_option('precision', 4)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "import sklearn.ensemble\n",
    "import sklearn.naive_bayes\n",
    "import sklearn.svm\n",
    "import sklearn.model_selection\n",
    "\n",
    "from numpy.core.umath_tests import inner1d\n",
    "\n",
    "from scipy.stats import uniform, expon\n",
    "\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV, validation_curve\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "\n",
    "from mi_clasificador_arbol_2 import MiClasificadorArbol\n",
    "from learning_curve import plot_learning_curve\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>...</th>\n",
       "      <th>193</th>\n",
       "      <th>194</th>\n",
       "      <th>195</th>\n",
       "      <th>196</th>\n",
       "      <th>197</th>\n",
       "      <th>198</th>\n",
       "      <th>199</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>index</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.4914</td>\n",
       "      <td>0.1644</td>\n",
       "      <td>1.2315</td>\n",
       "      <td>1.2429</td>\n",
       "      <td>1.5576</td>\n",
       "      <td>0.0455</td>\n",
       "      <td>0.1302</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.1983</td>\n",
       "      <td>-0.0118</td>\n",
       "      <td>1.5375</td>\n",
       "      <td>-0.7727</td>\n",
       "      <td>-0.1401</td>\n",
       "      <td>2.0871</td>\n",
       "      <td>-0.8312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.2749</td>\n",
       "      <td>0.2780</td>\n",
       "      <td>-1.3108</td>\n",
       "      <td>0.6801</td>\n",
       "      <td>-0.5503</td>\n",
       "      <td>0.6359</td>\n",
       "      <td>-0.4478</td>\n",
       "      <td>...</td>\n",
       "      <td>1.2190</td>\n",
       "      <td>-0.3190</td>\n",
       "      <td>-0.6446</td>\n",
       "      <td>-0.0061</td>\n",
       "      <td>-1.2374</td>\n",
       "      <td>-1.3291</td>\n",
       "      <td>-1.3265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.2243</td>\n",
       "      <td>-0.5710</td>\n",
       "      <td>-0.2712</td>\n",
       "      <td>-0.1328</td>\n",
       "      <td>-1.0045</td>\n",
       "      <td>0.9315</td>\n",
       "      <td>-1.4507</td>\n",
       "      <td>...</td>\n",
       "      <td>0.9459</td>\n",
       "      <td>0.1430</td>\n",
       "      <td>-0.1989</td>\n",
       "      <td>-0.0393</td>\n",
       "      <td>-0.5866</td>\n",
       "      <td>2.2507</td>\n",
       "      <td>1.4925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.5853</td>\n",
       "      <td>-0.8532</td>\n",
       "      <td>-0.2723</td>\n",
       "      <td>-0.5493</td>\n",
       "      <td>-2.9824</td>\n",
       "      <td>-0.1697</td>\n",
       "      <td>-0.0430</td>\n",
       "      <td>...</td>\n",
       "      <td>1.6488</td>\n",
       "      <td>-0.7363</td>\n",
       "      <td>-0.8866</td>\n",
       "      <td>-1.2717</td>\n",
       "      <td>-0.1493</td>\n",
       "      <td>0.2007</td>\n",
       "      <td>-1.4820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.4155</td>\n",
       "      <td>1.4187</td>\n",
       "      <td>0.6027</td>\n",
       "      <td>-0.7993</td>\n",
       "      <td>0.2939</td>\n",
       "      <td>-0.1796</td>\n",
       "      <td>-0.7140</td>\n",
       "      <td>...</td>\n",
       "      <td>1.1314</td>\n",
       "      <td>-0.4230</td>\n",
       "      <td>-0.2685</td>\n",
       "      <td>0.3045</td>\n",
       "      <td>-1.2245</td>\n",
       "      <td>-1.9421</td>\n",
       "      <td>1.5186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>0.2516</td>\n",
       "      <td>0.9375</td>\n",
       "      <td>-1.1980</td>\n",
       "      <td>0.4577</td>\n",
       "      <td>0.9287</td>\n",
       "      <td>0.5373</td>\n",
       "      <td>0.2476</td>\n",
       "      <td>...</td>\n",
       "      <td>0.5829</td>\n",
       "      <td>-0.5494</td>\n",
       "      <td>0.4607</td>\n",
       "      <td>1.2182</td>\n",
       "      <td>0.1025</td>\n",
       "      <td>3.0034</td>\n",
       "      <td>-0.0344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>0.6246</td>\n",
       "      <td>-1.0590</td>\n",
       "      <td>0.9491</td>\n",
       "      <td>0.2687</td>\n",
       "      <td>0.6610</td>\n",
       "      <td>-1.6657</td>\n",
       "      <td>0.3982</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.1075</td>\n",
       "      <td>0.8993</td>\n",
       "      <td>-0.4229</td>\n",
       "      <td>0.3977</td>\n",
       "      <td>-0.0808</td>\n",
       "      <td>-1.7054</td>\n",
       "      <td>-0.4786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497</th>\n",
       "      <td>0.2677</td>\n",
       "      <td>0.1802</td>\n",
       "      <td>0.7154</td>\n",
       "      <td>0.3542</td>\n",
       "      <td>-0.9023</td>\n",
       "      <td>-1.7792</td>\n",
       "      <td>-0.0121</td>\n",
       "      <td>...</td>\n",
       "      <td>0.8491</td>\n",
       "      <td>0.7469</td>\n",
       "      <td>0.2071</td>\n",
       "      <td>-1.0090</td>\n",
       "      <td>0.3317</td>\n",
       "      <td>-1.7513</td>\n",
       "      <td>-0.5397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>0.1926</td>\n",
       "      <td>0.7834</td>\n",
       "      <td>1.7056</td>\n",
       "      <td>0.3418</td>\n",
       "      <td>-0.8350</td>\n",
       "      <td>0.4068</td>\n",
       "      <td>0.0495</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.0130</td>\n",
       "      <td>0.1483</td>\n",
       "      <td>0.5019</td>\n",
       "      <td>-0.0020</td>\n",
       "      <td>-1.6642</td>\n",
       "      <td>2.5117</td>\n",
       "      <td>-0.0118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>0.0427</td>\n",
       "      <td>0.4028</td>\n",
       "      <td>-0.6085</td>\n",
       "      <td>1.0845</td>\n",
       "      <td>0.1033</td>\n",
       "      <td>0.2698</td>\n",
       "      <td>-0.8598</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.3587</td>\n",
       "      <td>-0.3121</td>\n",
       "      <td>-0.7630</td>\n",
       "      <td>0.6525</td>\n",
       "      <td>0.6161</td>\n",
       "      <td>-0.0902</td>\n",
       "      <td>-1.0215</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>500 rows × 200 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            0       1       2       3       4       5       6   ...       193  \\\n",
       "index                                                           ...             \n",
       "0      1.4914  0.1644  1.2315  1.2429  1.5576  0.0455  0.1302   ...   -1.1983   \n",
       "1     -0.2749  0.2780 -1.3108  0.6801 -0.5503  0.6359 -0.4478   ...    1.2190   \n",
       "2     -0.2243 -0.5710 -0.2712 -0.1328 -1.0045  0.9315 -1.4507   ...    0.9459   \n",
       "3      0.5853 -0.8532 -0.2723 -0.5493 -2.9824 -0.1697 -0.0430   ...    1.6488   \n",
       "4     -1.4155  1.4187  0.6027 -0.7993  0.2939 -0.1796 -0.7140   ...    1.1314   \n",
       "...       ...     ...     ...     ...     ...     ...     ...   ...       ...   \n",
       "495    0.2516  0.9375 -1.1980  0.4577  0.9287  0.5373  0.2476   ...    0.5829   \n",
       "496    0.6246 -1.0590  0.9491  0.2687  0.6610 -1.6657  0.3982   ...   -0.1075   \n",
       "497    0.2677  0.1802  0.7154  0.3542 -0.9023 -1.7792 -0.0121   ...    0.8491   \n",
       "498    0.1926  0.7834  1.7056  0.3418 -0.8350  0.4068  0.0495   ...   -0.0130   \n",
       "499    0.0427  0.4028 -0.6085  1.0845  0.1033  0.2698 -0.8598   ...   -0.3587   \n",
       "\n",
       "          194     195     196     197     198     199  \n",
       "index                                                  \n",
       "0     -0.0118  1.5375 -0.7727 -0.1401  2.0871 -0.8312  \n",
       "1     -0.3190 -0.6446 -0.0061 -1.2374 -1.3291 -1.3265  \n",
       "2      0.1430 -0.1989 -0.0393 -0.5866  2.2507  1.4925  \n",
       "3     -0.7363 -0.8866 -1.2717 -0.1493  0.2007 -1.4820  \n",
       "4     -0.4230 -0.2685  0.3045 -1.2245 -1.9421  1.5186  \n",
       "...       ...     ...     ...     ...     ...     ...  \n",
       "495   -0.5494  0.4607  1.2182  0.1025  3.0034 -0.0344  \n",
       "496    0.8993 -0.4229  0.3977 -0.0808 -1.7054 -0.4786  \n",
       "497    0.7469  0.2071 -1.0090  0.3317 -1.7513 -0.5397  \n",
       "498    0.1483  0.5019 -0.0020 -1.6642  2.5117 -0.0118  \n",
       "499   -0.3121 -0.7630  0.6525  0.6161 -0.0902 -1.0215  \n",
       "\n",
       "[500 rows x 200 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>output</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>index</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>500 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       output\n",
       "index        \n",
       "0           0\n",
       "1           0\n",
       "2           0\n",
       "3           0\n",
       "4           1\n",
       "...       ...\n",
       "495         1\n",
       "496         0\n",
       "497         1\n",
       "498         0\n",
       "499         0\n",
       "\n",
       "[500 rows x 1 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>...</th>\n",
       "      <th>193</th>\n",
       "      <th>194</th>\n",
       "      <th>195</th>\n",
       "      <th>196</th>\n",
       "      <th>197</th>\n",
       "      <th>198</th>\n",
       "      <th>199</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>500.0000</td>\n",
       "      <td>500.0000</td>\n",
       "      <td>500.0000</td>\n",
       "      <td>500.0000</td>\n",
       "      <td>500.0000</td>\n",
       "      <td>500.0000</td>\n",
       "      <td>500.0000</td>\n",
       "      <td>...</td>\n",
       "      <td>500.0000</td>\n",
       "      <td>500.0000</td>\n",
       "      <td>500.0000</td>\n",
       "      <td>500.0000</td>\n",
       "      <td>500.0000</td>\n",
       "      <td>500.0000</td>\n",
       "      <td>500.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.0384</td>\n",
       "      <td>0.0715</td>\n",
       "      <td>0.0056</td>\n",
       "      <td>-0.0103</td>\n",
       "      <td>-0.0436</td>\n",
       "      <td>-0.0208</td>\n",
       "      <td>-0.0571</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.0187</td>\n",
       "      <td>0.0087</td>\n",
       "      <td>-0.0356</td>\n",
       "      <td>-0.1940</td>\n",
       "      <td>0.0250</td>\n",
       "      <td>0.0257</td>\n",
       "      <td>-0.0036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.0153</td>\n",
       "      <td>0.9613</td>\n",
       "      <td>1.0360</td>\n",
       "      <td>1.0230</td>\n",
       "      <td>1.0647</td>\n",
       "      <td>0.9898</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.9731</td>\n",
       "      <td>0.9716</td>\n",
       "      <td>1.0075</td>\n",
       "      <td>1.0246</td>\n",
       "      <td>0.9934</td>\n",
       "      <td>0.9940</td>\n",
       "      <td>0.9819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-3.1722</td>\n",
       "      <td>-2.4596</td>\n",
       "      <td>-2.8834</td>\n",
       "      <td>-3.7474</td>\n",
       "      <td>-2.9987</td>\n",
       "      <td>-3.2014</td>\n",
       "      <td>-3.6855</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.9110</td>\n",
       "      <td>-2.9642</td>\n",
       "      <td>-2.5163</td>\n",
       "      <td>-3.9278</td>\n",
       "      <td>-2.4254</td>\n",
       "      <td>-2.6234</td>\n",
       "      <td>-2.8690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-0.6090</td>\n",
       "      <td>-0.6207</td>\n",
       "      <td>-0.7180</td>\n",
       "      <td>-0.6594</td>\n",
       "      <td>-0.7177</td>\n",
       "      <td>-0.6510</td>\n",
       "      <td>-0.7073</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.6441</td>\n",
       "      <td>-0.6509</td>\n",
       "      <td>-0.6613</td>\n",
       "      <td>-0.8689</td>\n",
       "      <td>-0.6466</td>\n",
       "      <td>-0.6354</td>\n",
       "      <td>-0.6855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.0602</td>\n",
       "      <td>0.0560</td>\n",
       "      <td>-0.0713</td>\n",
       "      <td>0.0612</td>\n",
       "      <td>-0.0097</td>\n",
       "      <td>-0.0407</td>\n",
       "      <td>-0.0771</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.0473</td>\n",
       "      <td>0.0537</td>\n",
       "      <td>-0.0749</td>\n",
       "      <td>-0.1901</td>\n",
       "      <td>0.0185</td>\n",
       "      <td>-0.0332</td>\n",
       "      <td>-0.0797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.6334</td>\n",
       "      <td>0.7670</td>\n",
       "      <td>0.7066</td>\n",
       "      <td>0.6699</td>\n",
       "      <td>0.6616</td>\n",
       "      <td>0.6508</td>\n",
       "      <td>0.6029</td>\n",
       "      <td>...</td>\n",
       "      <td>0.6071</td>\n",
       "      <td>0.6860</td>\n",
       "      <td>0.5743</td>\n",
       "      <td>0.4636</td>\n",
       "      <td>0.7041</td>\n",
       "      <td>0.6575</td>\n",
       "      <td>0.6608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2.9702</td>\n",
       "      <td>2.7920</td>\n",
       "      <td>2.6905</td>\n",
       "      <td>2.8091</td>\n",
       "      <td>2.9823</td>\n",
       "      <td>2.9342</td>\n",
       "      <td>3.3240</td>\n",
       "      <td>...</td>\n",
       "      <td>3.0983</td>\n",
       "      <td>3.1469</td>\n",
       "      <td>2.9109</td>\n",
       "      <td>2.4942</td>\n",
       "      <td>3.1804</td>\n",
       "      <td>3.0034</td>\n",
       "      <td>2.5107</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 200 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              0         1         2         3         4         5         6  \\\n",
       "count  500.0000  500.0000  500.0000  500.0000  500.0000  500.0000  500.0000   \n",
       "mean     0.0384    0.0715    0.0056   -0.0103   -0.0436   -0.0208   -0.0571   \n",
       "std      1.0153    0.9613    1.0360    1.0230    1.0647    0.9898    1.0000   \n",
       "min     -3.1722   -2.4596   -2.8834   -3.7474   -2.9987   -3.2014   -3.6855   \n",
       "25%     -0.6090   -0.6207   -0.7180   -0.6594   -0.7177   -0.6510   -0.7073   \n",
       "50%      0.0602    0.0560   -0.0713    0.0612   -0.0097   -0.0407   -0.0771   \n",
       "75%      0.6334    0.7670    0.7066    0.6699    0.6616    0.6508    0.6029   \n",
       "max      2.9702    2.7920    2.6905    2.8091    2.9823    2.9342    3.3240   \n",
       "\n",
       "         ...          193       194       195       196       197       198  \\\n",
       "count    ...     500.0000  500.0000  500.0000  500.0000  500.0000  500.0000   \n",
       "mean     ...      -0.0187    0.0087   -0.0356   -0.1940    0.0250    0.0257   \n",
       "std      ...       0.9731    0.9716    1.0075    1.0246    0.9934    0.9940   \n",
       "min      ...      -2.9110   -2.9642   -2.5163   -3.9278   -2.4254   -2.6234   \n",
       "25%      ...      -0.6441   -0.6509   -0.6613   -0.8689   -0.6466   -0.6354   \n",
       "50%      ...      -0.0473    0.0537   -0.0749   -0.1901    0.0185   -0.0332   \n",
       "75%      ...       0.6071    0.6860    0.5743    0.4636    0.7041    0.6575   \n",
       "max      ...       3.0983    3.1469    2.9109    2.4942    3.1804    3.0034   \n",
       "\n",
       "            199  \n",
       "count  500.0000  \n",
       "mean    -0.0036  \n",
       "std      0.9819  \n",
       "min     -2.8690  \n",
       "25%     -0.6855  \n",
       "50%     -0.0797  \n",
       "75%      0.6608  \n",
       "max      2.5107  \n",
       "\n",
       "[8 rows x 200 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Carga de datos\n",
    "X = pd.read_csv(\"../TP1/X.csv\", index_col=\"index\")\n",
    "y = pd.read_csv(\"../TP1/y.csv\", index_col=\"index\", dtype=int)  # Cargamos los valores booleanos (True y False)\n",
    "                                                        # como números (1 y 0) para facilitar el manejo luego. \n",
    "    \n",
    "X_competencia = pd.read_csv(\"../TP1/X_competencia1.csv\", index_col=\"index\")\n",
    "y_competencia_ejemplo = pd.read_csv(\"../TP1/y_competencia_ejemplo.csv\", index_col=\"index\")\n",
    "display(X)\n",
    "display(y)\n",
    "X.describe()\n",
    "\n",
    "# Descomentar si quieren ver los datos para la competencia:\n",
    "# display(X_competencia) \n",
    "# display(y_competencia_ejemplo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejercicio 1\n",
    "\n",
    "### Separación de datos\n",
    "\n",
    "Contarán con una cantidad limitada de datos, por lo cual es importante tomar una buena decisión en el momento de empezar a utilizarlos. En este punto pedimos que evalúen cómo separar sus datos para desarrollo y para evaluación tomando en cuenta la competencia. No consideramos necesario normalizar los datos. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_dev: (400, 200), y_dev: (400, 1) para desarrollo\n",
      "X_eval: (100, 200), y_eval: (100, 1) para evaluación\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x216 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "########################################################\n",
    "\n",
    "X_dev, X_eval, y_dev, y_eval = sklearn.model_selection.train_test_split(X, y, random_state=SEED, test_size=0.2)\n",
    "\n",
    "#########################################################\n",
    "\n",
    "print(\"X_dev: {}, y_dev: {} para desarrollo\".format(X_dev.shape, y_dev.shape))\n",
    "print(\"X_eval: {}, y_eval: {} para evaluación\".format(X_eval.shape, y_eval.shape))\n",
    "\n",
    "plt.figure(figsize=(5, 3))\n",
    "plt.hist(np.array(y_dev))  # muestra un histograma para la distribución de y.\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Del total de los datos nos quedamos con un 0.2% como held out que no fue utilizado a lo largo del trabajo práctico. A la hora de la separación tuvimos en cuenta la distribución de los datos para que fuese equivalente en ambos grupos. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejercicio 2\n",
    "\n",
    "### Construcción de modelos\n",
    "\n",
    "Para este punto, la tarea consiste en construir y evaluar modelos de tipo árbol de decisión, de manera de obtener una estimación realista de la performance de los mismos. \n",
    "\n",
    "1. Entrenar un árbol de decisión con altura máxima 3 y el resto de los hiperparámetros en default. \n",
    "2. Estimar la performance del modelo utilizando K-fold cross validation con K = 5, con las métricas “Accuracy” y “ROC AUC”. Para ello, se pide medir la performance en cada partición tanto sobre el fold de validación como sobre los folds de entrenamiento. Luego, completar la primera tabla.\n",
    "3. Entrenar árboles de decisión para cada una de las siguientes combinaciones y completar la segunda tabla.\n",
    "\n",
    "----\n",
    "\n",
    "**EJERCICIO EXTRA**: Usar la implementación de árboles de decisión que realizaron para la guía de ejercicios de la materia. Adaptarla para que cumpla con la interfaz requerida por sklearn, asegurarse de que funcione con variables continuas y reproducir las tablas anteriores.   \n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_dev_np = np.array(X_dev)\n",
    "y_dev_np = np.array(y_dev).ravel()\n",
    "\n",
    "kf = KFold(n_splits=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Aclaración**: en este punto dividiremos el *X_dev*, definido inicialmente resultado de la separación del **held-out**, en dos sets: uno llamado *train*, con el que entrenaremos los modelos; y otro llamado *test*, con el que validaremos los modelos (*cross-validation*). Por otro lado, en los casos en los que usamos *k-fold cross-validation* seleccionaremos los *folds* sobre *X_dev*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<h3> TABLA 1 </h3>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy (train)</th>\n",
       "      <th>Accuracy (val)</th>\n",
       "      <th>AUC ROC (train)</th>\n",
       "      <th>AUC ROC (val)</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Perm</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.8031</td>\n",
       "      <td>0.5875</td>\n",
       "      <td>0.8845</td>\n",
       "      <td>0.5870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.8500</td>\n",
       "      <td>0.6250</td>\n",
       "      <td>0.8813</td>\n",
       "      <td>0.6512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.8187</td>\n",
       "      <td>0.6125</td>\n",
       "      <td>0.8810</td>\n",
       "      <td>0.5918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.8281</td>\n",
       "      <td>0.7375</td>\n",
       "      <td>0.8618</td>\n",
       "      <td>0.7596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.8094</td>\n",
       "      <td>0.5750</td>\n",
       "      <td>0.8656</td>\n",
       "      <td>0.5954</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Accuracy (train)  Accuracy (val)  AUC ROC (train)  AUC ROC (val)\n",
       "Perm                                                                  \n",
       "1               0.8031          0.5875           0.8845         0.5870\n",
       "2               0.8500          0.6250           0.8813         0.6512\n",
       "3               0.8187          0.6125           0.8810         0.5918\n",
       "4               0.8281          0.7375           0.8618         0.7596\n",
       "5               0.8094          0.5750           0.8656         0.5954"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "accuracies_training, accuracies_validation, aucs_training, aucs_validation = [], [], [], []\n",
    "\n",
    "########################################################\n",
    "# (1) y (2)\n",
    "########################################################\n",
    "for dev_index, eval_index in kf.split(X_dev_np):\n",
    "    X_dev_folds, X_eval_folds = X_dev_np[dev_index], X_dev_np[eval_index]\n",
    "    y_dev_folds, y_eval_folds = y_dev_np[dev_index], y_dev_np[eval_index]\n",
    "    \n",
    "    tree = DecisionTreeClassifier(max_depth=3)\n",
    "    tree.fit(X_dev_folds, y_dev_folds)\n",
    "    \n",
    "    prediction_dev_c  = tree.predict(X_dev_folds) # _c es para cuando devuelve clase\n",
    "    prediction_eval_c = tree.predict(X_eval_folds)\n",
    "    \n",
    "    accuracies_training.append(accuracy_score(y_dev_folds, prediction_dev_c))\n",
    "    accuracies_validation.append(accuracy_score(y_eval_folds, prediction_eval_c))\n",
    "\n",
    "    prediction_dev_p  = tree.predict_proba(X_dev_folds)[:,1] # _p es para cuando devuelve probabilidades\n",
    "    prediction_eval_p = tree.predict_proba(X_eval_folds)[:,1]\n",
    "    \n",
    "    aucs_training.append(roc_auc_score(y_dev_folds, prediction_dev_p))\n",
    "    aucs_validation.append(roc_auc_score(y_eval_folds, prediction_eval_p))\n",
    "    \n",
    "#########################################################\n",
    "\n",
    "df = pd.DataFrame(index=range(1,6))\n",
    "df.index.name = \"Perm\"\n",
    "                  \n",
    "df[\"Accuracy (train)\"]   = accuracies_training   \n",
    "df[\"Accuracy (val)\"] = accuracies_validation  \n",
    "df[\"AUC ROC (train)\"]    = aucs_training      \n",
    "df[\"AUC ROC (val)\"]  = aucs_validation    \n",
    "\n",
    "\n",
    "display(HTML(\"<h3> TABLA 1 </h3>\"))\n",
    "display(df)\n",
    "\n",
    "df.plot(kind=\"bar\")\n",
    "plt.legend(loc='upper left', bbox_to_anchor=(1.0, 1.0))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como se puede observar, tanto los valores de Accuracy como los de AUC-ROC se mantienen estables entre los folds, tanto para el entrenamiento (train) como para la validación (val). Además, se observa una esperada baja en ambas métricas cuando pasamos de predecir los datos de entrenamiento a predecir los de validación. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(X_dev, y_dev, random_state=SEED, test_size=0.2)\n",
    "\n",
    "X_train_np = np.array(X_train)\n",
    "y_train_np = np.array(y_train).ravel()\n",
    "X_test_np  = np.array(X_test)\n",
    "y_test_np  = np.array(y_test).ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<h3> TABLA 2 </h3>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Altura máxima</th>\n",
       "      <th>Criterio de evaluación de corte</th>\n",
       "      <th>AUC ROC promedio (training)</th>\n",
       "      <th>AUC ROC promedio (validación)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>Gini</td>\n",
       "      <td>0.8832</td>\n",
       "      <td>0.6934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>Gini</td>\n",
       "      <td>0.9666</td>\n",
       "      <td>0.7201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Inifinito</td>\n",
       "      <td>Gini</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.6562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Ganancia de Información</td>\n",
       "      <td>0.8869</td>\n",
       "      <td>0.6888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Ganancia de Información</td>\n",
       "      <td>0.9865</td>\n",
       "      <td>0.6729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Inifinito</td>\n",
       "      <td>Ganancia de Información</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.6250</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Altura máxima Criterio de evaluación de corte  AUC ROC promedio (training)  \\\n",
       "0             3                            Gini                       0.8832   \n",
       "1             5                            Gini                       0.9666   \n",
       "2     Inifinito                            Gini                       1.0000   \n",
       "3             3         Ganancia de Información                       0.8869   \n",
       "4             5         Ganancia de Información                       0.9865   \n",
       "5     Inifinito         Ganancia de Información                       1.0000   \n",
       "\n",
       "   AUC ROC promedio (validación)  \n",
       "0                         0.6934  \n",
       "1                         0.7201  \n",
       "2                         0.6562  \n",
       "3                         0.6888  \n",
       "4                         0.6729  \n",
       "5                         0.6250  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "########################################################\n",
    "# (3)\n",
    "########################################################\n",
    "\n",
    "AUC_train, AUC_val = [], []\n",
    "\n",
    "for criterio in [\"gini\", \"entropy\"]:\n",
    "     for altura in [3, 5, None]:\n",
    "        tree = DecisionTreeClassifier(max_depth=altura, criterion=criterio, random_state=SEED)\n",
    "        tree.fit(X_train, y_train)\n",
    "        \n",
    "        pred_dev_p  = tree.predict_proba(X_train)[:,1]\n",
    "        pred_eval_p = tree.predict_proba(X_test)[:,1]\n",
    "        \n",
    "        AUC_train.append(roc_auc_score(y_train, pred_dev_p))\n",
    "        AUC_val.append(roc_auc_score(y_test, pred_eval_p))\n",
    "\n",
    "df = pd.DataFrame(index=range(0,6))\n",
    "\n",
    "df[\"Altura máxima\"] = [3, 5, \"Inifinito\"] * 2\n",
    "df[\"Criterio de evaluación de corte\"] = [\"Gini\"] * 3 + [\"Ganancia de Información\"] * 3\n",
    "df[\"AUC ROC promedio (training)\"] = AUC_train # reemplazar por resultados_training\n",
    "df[\"AUC ROC promedio (validación)\"] = AUC_val # reemplazar por resultados_validation\n",
    "\n",
    "   \n",
    "display(HTML(\"<h3> TABLA 2 </h3>\"))\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nuevamente, al igual en en el punto anterior, se observa una mejor predicción sobre los datos vistos en el entrenamiento con respecto a los datos no vistos en el entrenamiento. Con respecto a los parámetros variados (altura máxima y criterio de evaluación de corte) podemos ver que: (1) la profundidad del árbol es determinante para el desempeño de nuestra clasificación, llegando incluso a modelos overfitteados cuando permitimos demasiada flexibilidad (mayor profundidad) a los mismos; y (2) no existe mucha diferencia entre utilizar Gini o Information Gain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Extra] Mi Clasificador Árbol\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<h3> TABLA 1 </h3>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy (training)</th>\n",
       "      <th>Accuracy (validación)</th>\n",
       "      <th>AUC ROC (training)</th>\n",
       "      <th>AUC ROC (validación)</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Permutación</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.7531</td>\n",
       "      <td>0.5875</td>\n",
       "      <td>0.8243</td>\n",
       "      <td>0.6016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.7750</td>\n",
       "      <td>0.6125</td>\n",
       "      <td>0.8380</td>\n",
       "      <td>0.6691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.7469</td>\n",
       "      <td>0.6750</td>\n",
       "      <td>0.8086</td>\n",
       "      <td>0.6857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.7500</td>\n",
       "      <td>0.7500</td>\n",
       "      <td>0.8013</td>\n",
       "      <td>0.7769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.7781</td>\n",
       "      <td>0.6125</td>\n",
       "      <td>0.8045</td>\n",
       "      <td>0.6911</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Accuracy (training)  Accuracy (validación)  AUC ROC (training)  \\\n",
       "Permutación                                                                   \n",
       "1                         0.7531                 0.5875              0.8243   \n",
       "2                         0.7750                 0.6125              0.8380   \n",
       "3                         0.7469                 0.6750              0.8086   \n",
       "4                         0.7500                 0.7500              0.8013   \n",
       "5                         0.7781                 0.6125              0.8045   \n",
       "\n",
       "             AUC ROC (validación)  \n",
       "Permutación                        \n",
       "1                          0.6016  \n",
       "2                          0.6691  \n",
       "3                          0.6857  \n",
       "4                          0.7769  \n",
       "5                          0.6911  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from mi_clasificador_arbol_2 import MiClasificadorArbol\n",
    "\n",
    "accuracies_training, accuracies_validation, aucs_training, aucs_validation = [], [], [], []\n",
    "\n",
    "for dev_index, eval_index in kf.split(X_dev_np):\n",
    "    X_dev_folds  = X_dev_np[dev_index]\n",
    "    X_eval_folds = X_dev_np[eval_index]\n",
    "    y_dev_folds, y_eval_folds = y_dev_np[dev_index], y_dev_np[eval_index]\n",
    "    tree = MiClasificadorArbol(list(X.columns), profundidad_max=3, criterion='gini')\n",
    "    tree.fit(X_dev_folds, y_dev_folds)\n",
    "\n",
    "    prediction_dev_c = tree.predict(X_dev_folds)\n",
    "    prediction_eval_c = tree.predict(X_eval_folds)\n",
    "\n",
    "    accuracies_training.append(accuracy_score(y_dev_folds, prediction_dev_c))\n",
    "    accuracies_validation.append(accuracy_score(y_eval_folds, prediction_eval_c))\n",
    "\n",
    "    prediction_dev_p = tree.predict_proba(X_dev_folds)\n",
    "    prediction_eval_p = tree.predict_proba(X_eval_folds)\n",
    "\n",
    "    aucs_training.append(roc_auc_score(y_dev_folds, prediction_dev_p))\n",
    "    aucs_validation.append(roc_auc_score(y_eval_folds, prediction_eval_p))\n",
    "\n",
    "## Objetivo: accuracies_training, accuracies_validation, aucs_training y aucs_validation asignados\n",
    "#########################################################\n",
    "\n",
    "df = pd.DataFrame(index=range(1,6))\n",
    "df.index.name = \"Permutación\"\n",
    "                  \n",
    "df[\"Accuracy (training)\"] = accuracies_training   \n",
    "df[\"Accuracy (validación)\"] = accuracies_validation  \n",
    "df[\"AUC ROC (training)\"] = aucs_training      \n",
    "df[\"AUC ROC (validación)\"] = aucs_validation    \n",
    "\n",
    "display(HTML(\"<h3> TABLA 1 </h3>\"))\n",
    "display(df)\n",
    "\n",
    "# Descomentar las siguientes líneas para graficar el resultado\n",
    "df.plot(kind=\"bar\")\n",
    "plt.legend(loc='upper left', bbox_to_anchor=(1.0, 1.0))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ########################################################\n",
    "# # (EE 3)\n",
    "# ########################################################\n",
    "\n",
    "# resultados_training = []\n",
    "# resultados_validation = []\n",
    "\n",
    "# for criterio in [\"gini\", \"entropy\"]:\n",
    "#     for altura in [3, 5, None]:\n",
    "#         tree = MiClasificadorArbol(list(X.columns), profundidad_max=altura, criterion=criterio)\n",
    "#         tree.fit(X_train_np, y_train_np)\n",
    "        \n",
    "#         prediction_dev_p  = tree.predict_proba(X_train_np)\n",
    "#         prediction_eval_p = tree.predict_proba(X_test_np)\n",
    "\n",
    "#         resultados_training.append(roc_auc_score(y_train, prediction_dev_p))\n",
    "#         resultados_validation.append(roc_auc_score(y_test, prediction_eval_p))\n",
    "\n",
    "# df = pd.DataFrame(index=range(0,6))\n",
    "\n",
    "# df[\"Altura máxima\"] = [3, 5, \"Inifinito\"] * 2\n",
    "# df[\"Criterio de evaluación de corte\"] = [\"Gini\"] * 3 + [\"Ganancia de Información\"] * 3\n",
    "# df[\"AUC ROC promedio (training)\"] = resultados_training # reemplazar por resultados_training\n",
    "# df[\"AUC ROC promedio (validación)\"] = resultados_validation # reemplazar por resultados_validation\n",
    "\n",
    "   \n",
    "# display(HTML(\"<h3> TABLA 2 </h3>\"))\n",
    "# display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejercicio 3: Comparación de algoritmos\n",
    "\n",
    "\n",
    "Se pide explorar distintas combinaciones de algoritmos de aprendizaje e hiperparámetros, de manera de buscar una performance óptima. Para este ejercicio es necesario que evalúen posibilidades utilizando la técnica de Grid Search. Como métrica de performance, usar siempre el área bajo la curva (AUC ROC) resultante de 5-fold cross-validation. \n",
    "\n",
    "Algoritmos a probar: KNN, árboles de decisión, LDA, Naive Bayes y SVM. Hiperparámetros: Revisar la documentación de cada uno para la búsqueda de combinaciones prometedoras.  \n",
    "\n",
    "Se pide generar un reporte que contenga: \n",
    "\n",
    "1. Una descripción de las distintas combinaciones consideradas y su performance asociada (las que consideren relevantes, con al menos la mejor combinación para cada algoritmo). \n",
    "\n",
    "1. Una breve explicación de los factores que creen que produjeron dicho resultado. \n",
    "\n",
    "En este punto evaluaremos tanto los hiperparámetros elegidos como las conclusiones relacionadas a por qué piensan que ciertos algoritmos funcionan mejor que otros para estos datos. \n",
    "\n",
    "\n",
    "\n",
    "----\n",
    "\n",
    "**EJERCICIO EXTRA**: Utilizar RandomizedSearchCV con rangos de parámetros que contengan a los utilizados en el GridSearch. Analizar si se encontraron mejores combinaciones de parámetros que no hayan sido tenidas en cuenta con el GridSearch y cuál fue la diferencia de tiempo de ejecución. \n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_resultados(grid, top=5, algorithm_name=''):\n",
    "    ## Si quieren, pueden utilizar esta función para imprimir las mejores combinaciones de su grid\n",
    "    print(algorithm_name)\n",
    "    print(\"Top {} combinaciones\".format(top))\n",
    "    df = pd.DataFrame(grid.cv_results_[\"params\"])\n",
    "    df[\"mean_score_validation\"] = grid.cv_results_[\"mean_test_score\"]\n",
    "    df[\"mean_score_training\"] = grid.cv_results_[\"mean_train_score\"]\n",
    "    display(df.sort_values(by=\"mean_score_validation\", ascending=False).head(top))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_np = np.array(X_dev)\n",
    "y_np = np.array(y_dev).ravel()\n",
    "n_iter = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "values_exponencial_distribution = np.int32(2 ** np.arange(0, 7, 0.001))\n",
    "plt.hist(np.array(values_exponencial_distribution))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "values_uniform_distribution = np.random.uniform(0, 1, 1000)\n",
    "plt.hist(np.array(values_uniform_distribution))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "values_log_distribution = np.logspace(-5, 2, 1000)\n",
    "plt.hist(np.array(values_log_distribution))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNeighbors Classifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "parameters_knn = {\n",
    "    'n_neighbors': np.unique(np.int32(2 ** np.arange(0, 7, 0.25))), \n",
    "    'algorithm': ['ball_tree', 'kd_tree', 'brute', 'auto']\n",
    "}\n",
    "clf = GridSearchCV(KNeighborsClassifier(weights='uniform'), parameters_knn, cv=5)\n",
    "clf.fit(X_np, y_np)\n",
    "top_resultados(clf, 88, 'KNeighbors Classifier - Grid Search')\n",
    "\n",
    "parameters_knn = {\n",
    "    'n_neighbors': np.int32(2 ** np.arange(0, 7, 0.001)), \n",
    "    'algorithm': ['ball_tree', 'kd_tree', 'brute', 'auto']\n",
    "}\n",
    "\n",
    "clf = RandomizedSearchCV(KNeighborsClassifier(weights='uniform'), parameters_knn, cv=5, n_iter=n_iter)\n",
    "clf.fit(X_np, y_np)\n",
    "top_resultados(clf, n_iter, 'KNeighbors Classifier - Randomized Search')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Del total de 8 parametros probamos todas las permutaciones de los siguientes tres hiperparámetros para distintos valores:\n",
    "\n",
    "- *n_neighbors*: número de vecinos utilizado por kneighbors query. Que por default es 5. \n",
    "- *weights*: es la función de peso que se utiliza en la predicción. Tiene como opciones: uniform (todos los puntos del vecindario pesan lo mismo), distance (los vecinos más cercanos tienen más peso) y callable (función definida por el usuario). \n",
    "- *algorithm*: es el algoritmo utilizado para computar los vecinos más cercanos. Puede ser: ball_tree, kd_tree, brute o auto. \n",
    "\n",
    "En una primera entrega del trabajo habíamos elegido hacer una búsqueda con Grid Search en un espacio lineal en un rango entre 1 y 107. En esta nueva entrega, la búsqueda consiste en un barrido de valores sobre una curva exponencial que a medida que crece va aumentando notablemente la cantidad de vecinos que prueba. El fundamento detrás de esto es que es conveniente probar valores distantes entre sí de vecinos para observar diferencias en los resultados del algoritmo. *n_neighbors* en la primera De las opciones de *weights* se dejó afuera *callable* por falta de experiencia. De *algorithm* se corrieron las opciones *ball_tree, kd_tree, brute y auto*.\n",
    "\n",
    "Respecto de la performance. Los mejores rendimientos están dados por las combinaciones: auto/brute/kd_tree, 22, uniform. En todas ellas el accuracy promedio sobre los datos de validación es de alrededor de **0.77**, mientras que el accuracy promedio sobre los datos de entrenamiento es cercano a **0.79**. En las otras combinaciones el primer valor baja o se mantiene, mientras que el accuracy del training llega a 1, clara muestra de *overfitting*.\n",
    "\n",
    "Pareciera que lo que marca la diferencia es el *weight*. Siempre mejora con uniform mientras que sobre ajusta con distance bajo las mismas combinaciones de los otros parámetros. \n",
    "\n",
    "Finalmente, se buscó optimizar la búsqueda de los hiperparámetros a través de Randomized Search. Ahora sí, sobre una distribución exponencial se tomó valores al azar. Lo esperable para estos casos es que la mayor cantidad de valores provengan de la primera parte de la exponencial que es la que crece más lento. Luego de 100 iteraciones se vió que la combinación de hiperparámetros para este modelo es con *algorithm* igual a *kd_tree*, la cantidad de vecinos 25 y *weigths* igual a *uniform*, con valores de *accuracy* similares a lo expresado previamente. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Discriminant Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "parameters_LDA = {\n",
    "    'shrinkage': [0, 1, 'auto'],\n",
    "}\n",
    "\n",
    "clf = GridSearchCV(LinearDiscriminantAnalysis(solver='lsqr'), parameters_LDA, cv=5)\n",
    "clf.fit(X_np, y_np)\n",
    "top_resultados(clf, 3, 'Linear Discriminant Analysis - Grid Search')\n",
    "\n",
    "distributions_LDA = {\n",
    "    'shrinkage': np.random.uniform(0, 1, 1000)\n",
    "}\n",
    "\n",
    "clf = RandomizedSearchCV(LinearDiscriminantAnalysis(solver='lsqr'), distributions_LDA, cv=5, n_iter=n_iter)\n",
    "clf.fit(X_np, y_np)\n",
    "top_resultados(clf, n_iter, 'Linear Discriminant Analysis - Randomized Search')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El parámetro del modelo denominado 'shrinkage' se utiliza para mejorar la estimacion de las matrices de covarianza en casos en que el numero de ejemplos de entrenamiento es chico en relacion a la cantidad de features. Puede variar entre 0 y 1. Si es 0 se utiliza la matriz de covarianza obtenida a partir de los features. Si es 1, utiliza una matriz diagonal donde la diagonal son las varianzas de cada feature. Esto define la forma de la normal. Cuanto mas valores tengas por fuera de la diagonal mas rotada puede estar la distribucion. \n",
    "Lo que hicimos fue usar auto, y un barrido lineal entre 0 y 1 y para randomizedSearch un barrido uniforme.\n",
    "\n",
    "Se observa que el modelo alcanza como máximo un accuracy promedio de **0.77** para los parámetros que se utilizan para su entrenamiento."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "parameters_tree = {\n",
    "    'criterion': ('gini', 'entropy'), \n",
    "    'splitter': ('random', 'best'), \n",
    "    'max_depth': np.unique(np.int32(2 ** np.arange(0, 7, 0.25)))\n",
    "}\n",
    "clf = GridSearchCV(DecisionTreeClassifier(), parameters_tree, cv=5)\n",
    "clf.fit(X_np, y_np)\n",
    "top_resultados(clf, 88, 'Decision Tree Classifier - Grid Search')\n",
    "\n",
    "distributions_tree = {\n",
    "    'criterion': ('gini', 'entropy'), \n",
    "    'splitter': ('random', 'best'), \n",
    "    'max_depth': np.int32(2 ** np.arange(0, 7, 0.001))\n",
    "}\n",
    "\n",
    "clf = RandomizedSearchCV(DecisionTreeClassifier(), distributions_tree, n_iter=n_iter, cv=5)\n",
    "clf.fit(X_np, y_np)\n",
    "top_resultados(clf, n_iter, 'Decision Tree Classifier - Randomized Search')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Del total de 13 parámetros probamos todas las permutaciones de los siguientes tres hiperparámetros para distintos valores:\n",
    "- *criterion*: es la función que mide la calidad del split. Las opciones son gini y entropy. \n",
    "- *splitter*: es la estrategia utilizada para elegir la partición en cada nodo. Puede ser best, para elegir el mejor corte o random que elige el mejor corte al azar. \n",
    "- *max_depth*: es la profundidad máxima del árbol. Puede ser seteada como int o como None. Si es None, los nodos se expanden hasta que todas las hojas son puras o hasta que contengan menos muestras que min_samples_split.\n",
    "\n",
    "En una primera corrida los valores utilizados para *criterion* fueron *entropy* y *gini*, mientras que para *splitter* fueron *random* y *best*. Para el hiperparámetro *max_depth* buscamos con Grid Search entre los valores de una curva exponencial. A diferencia de la entrega anterior, lo que intentamos explorar en este caso fue la variación de los resultados con valores más diferenciados de profundidad bajo la hipótesis de que la diferencia entre un árbol de altura x y otro de altura x+1 es menor a la existente entre uno de altura x y otro de x**2.\n",
    "\n",
    "Respecto de la performance, los mejores rendimientos estuvieron dados por las combinaciones gini/random/3 con un accuracy promedio para los datos de validación cercanos a **0.7** y con un accuracy promedio para los datos de entrenamiento cercanos a **0.77**. Para los valores muy altos de profundidad, se observan valores de *mean_score_training* de **1.00**. En estos casos, el modelo está sobreajustando sobre los datos de entrenamiento. \n",
    "\n",
    "Finalmente, en Randomized Search, se explora una variación de los valores de *max_depth* en una distribución exponencial. Nuevamente ocurre como en el caso anterior de Grid Search que los valores grandes tienden sobre ajustar y valores cercanos a 4, que varían levemente su performance dependiendo del *splitter* utilizado, dan buenos resultados. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gaussian Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = GridSearchCV(GaussianNB(), {}, cv=5)\n",
    "clf.fit(X_np, y_np)\n",
    "top_resultados(clf, 1, 'Gaussian')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este algoritmo no existen hiperparámetros que ajustar. Además, no se probará RandomizedSearch, ya que no hay parámetros con los cuales probar variantes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters_svm = {\n",
    "    'C': [100, 10, 1, 0.1, 0.01, 0.0001, 0.00001]\n",
    "}\n",
    "clf = GridSearchCV(LinearSVC(), parameters_svm, cv=5)\n",
    "clf.fit(X_np, y_np)\n",
    "top_resultados(clf, 7, 'Linear SVM - Grid Search')\n",
    "\n",
    "distributions_svm = {\n",
    "    'C': np.logspace(-5, 2, 1000)\n",
    "}\n",
    "\n",
    "clf = RandomizedSearchCV(LinearSVC(), distributions_svm, n_iter=n_iter, cv=5)\n",
    "clf.fit(X_np, y_np)\n",
    "top_resultados(clf, n_iter, 'Linear SVM - Randomized Search')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Variaremos el hiperparámetro *C* de modelo utilizando el *kernel* *lineal* por disposición de la consigna. \n",
    "\n",
    "El parametro *C* regula la proporción de clasificaciones incorrectas que se va a permitir a un determinado SVM. Valores de C más grandes resultarán en un hiperplano de margen más pequeño. \n",
    "\n",
    "Se varió C utilizando los valores [100, 10, 1, 0.1, 0.01, 0.0001, 0.00001] en Grid Search a lo largo de distintas iteraciones, observando que el *accuracy* sobre los datos de entrenamiento disminuyen al achicarse este valor. Esta mejora tiene su límite al llegar a valores de C muy bajos (alrededor de 1e-5), donde comienza a empeorar la clasificación. Esto se debe a que el entrenamiento permite encontrar hiperplanos muy permisivos con valores mal clasificados.\n",
    "\n",
    "A continuación corrimos la búsqueda con Randomized Search para un rango de valores de C entre 0.00001 y 100 con distribución logarítmica, tomando 100 valores. Observamos nuevamente un comportamiento similar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Conclusiones Grid Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Distintos sets de datos pueden ser mejor clasificados por unos u otros algoritmos. En este caso, las performances sobre los datos de validación siguen este orden decreciente: KNN, SVM, LDA, NB, DecisionTree.\n",
    "\n",
    "Tanto Naive Bayes como LDA cuentan con la desventaja de hacer suposiciones sobre la distribución de los datos, por lo que es posible que estas suposiciones no se cumplan y que sea ésta la causa de que no se encuentren entre los mejores modelos.\n",
    "\n",
    "Por otro lado, los otros tres son aquellos que no hacen suposiciones probabilísticas sobre los datos sino que se entrenan subdividiendo el espacio en el que se encuentran. Específicamente, aquellos que lo dividen de forma más compleja son los que generan mejor performance (SVM y KNN), mientras que los árboles de decisión, que dividen el espacio sólo con cortes rectos, fue el peor de los tres. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame()\n",
    "\n",
    "df[\"Model\"] = ['LDA', 'Decision Tree', 'Naive Bayes', 'SVM','KNN']\n",
    "df[\"mean_score_validation\"] = [0.7625, 0.6975, 0.73, 0.7700, 0.7750] # revisar! :)\n",
    "df[\"mean_score_training\"] = [0.9331, 0.8956, 0.7788, 0.9381, 0.7882]\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ejercicio 4: \n",
    "### Diagnóstico Sesgo-Varianza. \n",
    "\n",
    "En este punto, se pide inspeccionar dos de sus mejores modelos encontrados hasta ahora: el mejor modelo de tipo árbol de decisión y el mejor de tipo SVM. Para ello:\n",
    "\n",
    "1. Graficar curvas de complejidad para cada modelo, variando la profundidad en el caso de árboles, y el hiperparámetro C en el caso de SVM. Diagnosticar cómo afectan al sesgo y a la varianza esos dos hiperparámetros.\n",
    "2. Graficar curvas de aprendizaje para cada modelo. En base a estas curvas, sacar conclusiones sobre si los algoritmos parecen haber alcanzado su límite, o bien si aumentar la cantidad de datos debería ayudar.\n",
    "3. Construir un modelo RandomForest con 200 árboles. Explorar para qué sirve el hiperparámetro max_features y cómo afecta a la performance del algoritmo mediante una curva de complejidad. Explicar por qué creen que se dieron los resultados obtenidos. Por último, graficar una curva de aprendizaje sobre los parámetros elegidos para determinar si sería útil o no conseguir más datos (usar  grid search para encontrar una buena combinación de parámetros).  \n",
    "\n",
    "\n",
    "**Atención**: Tener en cuenta que debemos seguir utilizando ROC AUC como métrica para estas curvas.\n",
    "\n",
    "**ver**: http://scikit-learn.org/stable/modules/learning_curve.html#learning-curve\n",
    "\n",
    "----\n",
    "**EJERCICIO EXTRA:** Utilizar RandomizedSearchCV para explorar la performance del algoritmo de Gradient Boosting y comparar con los resultados obtenidos en el punto (c).\n",
    "\n",
    "\n",
    "----\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "repetitions = 50\n",
    "for i in range(repetitions):\n",
    "    max_depths = np.unique(np.int32(2 ** np.arange(0, 7, 0.25)))\n",
    "    auc_roc_train, auc_roc_test = [], []\n",
    "    for max_depth in max_depths:\n",
    "        tree = DecisionTreeClassifier(splitter='random', max_depth=max_depth)\n",
    "        tree.fit(X_train_np, y_train_np)\n",
    "\n",
    "        prediction_eval = tree.predict_proba(X_train_np)[:,1]\n",
    "        auc_roc_train.append(roc_auc_score(y_train_np, prediction_eval))\n",
    "\n",
    "        prediction_eval = tree.predict_proba(X_test_np)[:,1]\n",
    "        auc_roc_test.append(roc_auc_score(y_test_np, prediction_eval))\n",
    "    \n",
    "    results.append((auc_roc_train, auc_roc_test))\n",
    "    \n",
    "u = np.array(results).mean(axis=0) # promedio\n",
    "s = np.array(results).std(axis=0) # desviación\n",
    "\n",
    "plt.fill_between(max_depths, u[0]-s[0], u[0]+s[0], color=\"red\", alpha=0.3)\n",
    "plt.fill_between(max_depths, u[1]-s[1], u[1]+s[1], color=\"green\", alpha=0.3)\n",
    "\n",
    "plt.plot(max_depths, u[0], marker=\"*\", color=\"red\")\n",
    "plt.plot(max_depths, u[1], marker=\"*\", color=\"green\")\n",
    "\n",
    "plt.xscale('log')\n",
    "plt.title(\"Curva de complejidad para un modelo de Decision Tree Classifier\")\n",
    "plt.xlabel(\"Profundidad del árbol\")\n",
    "plt.ylabel(\"ROC-AUC\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "Cs = [100, 10, 1, 0.1, 0.01, 0.0001, 0.00001]\n",
    "auc_roc_train, auc_roc_test = [], []\n",
    "\n",
    "for C in Cs:\n",
    "    svm = LinearSVC(C=C)\n",
    "    svm.fit(X_train_np, y_train_np)\n",
    "\n",
    "    decision = svm.decision_function(X_train_np)\n",
    "    auc_roc_train.append(roc_auc_score(y_train_np, decision)) \n",
    "    \n",
    "    decision = svm.decision_function(X_test_np)\n",
    "    auc_roc_test.append(roc_auc_score(y_test_np, decision)) \n",
    "\n",
    "fig = plt.plot(Cs, auc_roc_train, marker=\"*\", color=\"red\")\n",
    "fig = plt.plot(Cs, auc_roc_test, marker=\"*\", color=\"green\")\n",
    "plt.xscale('log')\n",
    "plt.title(\"Curva de complejidad para un modelo de Support Vector Machine\")\n",
    "plt.xlabel(\"C\")\n",
    "plt.ylabel(\"ROC-AUC\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los errores de predicción pueden ser clasificados en errores de sesgo, errores de varianza y errores irreducibles. Los dos primeros pueden ser mejorados. Para hacerlo, el objetivo es que el método de aprendizaje logre disminuir al mismo tiempo sesgo y varianza. Se entiende por sesgo las suposiciones que los algoritmos hacen para poder estimar la función target que se desee aprender. Por varianza, la cantidad que varía la estimación al cambiar el set de datos de entrenamiento. Lo que suele ocurrir es que métodos más flexibles en cuanto a las suposiciones, es decir, con menor sesgo, resultan en una mayor varianza y, al revés. Se incurre, así, en un problema de trade-off entre sesgo y varianza. \n",
    "\n",
    "En el caso de los árboles de decisión, este trade-off depende de la profundidad del árbol, es decir, del parámetro max_depth. Un árbol de mayor profundidad hará muy pocas suposiciones sobre los datos y se adaptará muy bien a lo que le sea pasado como entrenamiento, aumentando así la varianza en gran medida al actuar sobre otro set de datos. \n",
    "En caso de los Support Vector Machines, el trade-off depende del parámetro C. Un aumento de este parámetro permite hacer más suposiciones sobre los datos achicando el margen de missclasification del hiperplano. Esto aumenta el sesgo y por consiguiente reduce la varianza.\n",
    "\n",
    "Para observar esto en nuestros datos, se relizaron gráfico AUC-ROC vs profundidas del árbol para el caso de Decision Tree y AUC-ROC vs C para SVM. Graficando en cada caso los dos tipos de error, de manera tal de observar el rendimiento del modelo con la variación de los parámetros. \n",
    "\n",
    "En ambos casos el comportamiento observado es el esperable: \n",
    "Para los árboles, el sombreado se corresponde a las variaciones de las distintas corridas consecuencia del splitter seteado en random, se ve que partir de una profundidad 3 aproximadamente el rendimiento disminuye. \n",
    "Para SVM no se ven variaciones demasiado significativas. La conclusión que se puede obtener es que para evitar overfitting se decide tomar valores más pequeños de C. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title = \"Learning Curves (SVM)\"\n",
    "estimator = LinearSVC(C=0.00001)\n",
    "plot_learning_curve(estimator, title, X_dev_np, y_dev_np, ylim=(0.75, 1.05), cv=5, n_jobs=4, scoring='roc_auc')\n",
    "\n",
    "title = \"Learning Curves (Decision Tree Classifier)\"\n",
    "estimator = DecisionTreeClassifier(max_depth=3, criterion='gini', splitter='random')\n",
    "plot_learning_curve(estimator, title, X_dev_np, y_dev_np, (0.4, 1.05), cv=5, n_jobs=4, scoring='roc_auc')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La curva de aprendizaje muestra el score de validación y de entrenamiento de un estimador variando el número de las muestras de entrenamiento. Sirve para averiguar cuanto se beneficia el modelo de agregar nueva data de entrenamiento y si el estimador sufre más de error de sesgo o de varianza. Lo deseable sería que las dos líneas del gráfico converjan al mismo valor a medida que aumenta el tamaño del set de entrenamiento. En este caso, para el primer gráfico, a partir de 175 aproximadamente no sería necesario seguir aumentando los datos para obtener información relevante, mientras que en el segundo, agregar más datos podría llegar a disminuir la distancia entre las dos curvas. En particular podría aumentar la línea verde para así obtener un mejor modelo. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters_rf = {\n",
    "    'n_estimators': np.random.randint(10, 51, 1000), \n",
    "    'max_features':  np.random.randint(10, 150, 1000),\n",
    "    'criterion': ('gini', 'entropy')\n",
    "}\n",
    "n_iter = 100\n",
    "clf = RandomizedSearchCV(RandomForestClassifier(max_depth=3), parameters_rf, n_iter, cv=5) # se fija max_depth=3 por los resultados vistos previamente\n",
    "clf.fit(X_np, y_np) \n",
    "top_resultados(clf, n_iter, 'RandomForestClassifier - Randomized Search')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "max_features = np.unique(np.int32(2 ** np.arange(3, 7, 0.25))) # va de 8 (un toque más que log2(200)) hasta 128 (un toque menos que la cant de features)\n",
    "train_scores, dev_scores = validation_curve(RandomForestClassifier(n_estimators=21, criterion='entropy', max_depth=3), X_dev, np.ravel(y_dev), \"max_features\", max_features, cv=5, scoring='roc_auc')\n",
    "\n",
    "train_mean = np.mean(train_scores, axis=1)\n",
    "train_std = np.std(train_scores, axis=1)\n",
    "\n",
    "dev_mean = np.mean(dev_scores, axis=1)\n",
    "dev_std = np.std(dev_scores, axis=1)\n",
    "\n",
    "plt.plot(max_features, train_mean, label=\"Training score\", color=\"red\")\n",
    "plt.plot(max_features, dev_mean, label=\"Cross-validation score\", color=\"green\")\n",
    "\n",
    "plt.fill_between(max_features, train_mean - train_std, train_mean + train_std, color=\"red\", alpha=0.3)\n",
    "plt.fill_between(max_features, dev_mean - dev_std, dev_mean + dev_std, color=\"green\", alpha=0.3)\n",
    "\n",
    "plt.title(\"Curva de complejidad con Random Forest\")\n",
    "plt.xlabel(\"Cantidad de features\")\n",
    "plt.ylabel(\"ROC AUC\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El método Random Forest crea una serie de árboles de decisión. Su particularidad reside en que, al momento de hacerlo, cada vez que se considera un split, se elige como candidatos del split de manera random una muestra m de predictores de entre la totalidad del set p. Y de estos m elegidos al azar, el split solo puede elegir uno. Esto hace que una nueva muestra de predictores sea elegida cada vez. \n",
    "\n",
    "max_features determina el número de features considerado al momento de buscar el mejor split. Puede ocurrir: \n",
    "\n",
    "- Si es un int, entonces considera la cantidad max_features de features en cada split. \n",
    "- Si es float, entonces la cantidad max_features es una fracción fraction y se considera int(max_features * n_features) en cada split. \n",
    "- Si es sqrt, entonces max_features=sqrt(n_features)\n",
    "- Si es log2, entonces max_features=log2(n_features)\n",
    "- Si es None, entonces max_features=n_features\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se realizó una curva de complejidad para graficar ROC AUC vs cantidad de features. Esto permitió ver cómo afectan las variaciones de Max_Depth a la performance del modelo. De la curva verde, se observa como se mantiene estable la media de perfomance del modelo a medida que aumenta la cantidad de features. Esto es, que el modelo no mejora significativamente si estos crecen en número. De la curva roja, en cambio, se observa como a medida que aumenta max_features aparece una tendencia al sobre ajuste.\n",
    "\n",
    "Por último, se grafica una curva de aprendizaje. Lo que se puede ver es que tanto el training como el cross validation score aumentan y se mantienen paralelos entre sí. Esto indica que el modelo puede beneficiarse de mayor cantidad de datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title = \"Learning Curves (Random Forest)\"\n",
    "estimator = RandomForestClassifier(n_estimators=200, criterion='gini', max_depth=3)\n",
    "plot_learning_curve(estimator, title, X_dev_np, y_dev_np, (0.7, 1.15), cv=5, n_jobs=4, scoring='roc_auc')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Boosting Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.analyticsvidhya.com/blog/2016/02/complete-guide-parameter-tuning-gradient-boosting-gbm-python/\n",
    "\n",
    "parameters_gb = {\n",
    "    'min_samples_split': np.random.randint(2, 50, 20),\n",
    "    'min_samples_leaf': np.random.randint(20, 150, 50),\n",
    "    'n_estimators': np.random.randint(2, 150, 1000), \n",
    "    'max_features':  np.random.randint(10, 150, 1000), \n",
    "    'loss' : ['deviance', 'exponential'],\n",
    "}\n",
    "\n",
    "n_iter = 100\n",
    "clf = RandomizedSearchCV(GradientBoostingClassifier(max_depth=3, subsample=0.8), parameters_gb, n_iter, cv=5)\n",
    "clf.fit(X_np, y_np)\n",
    "top_resultados(clf, n_iter, 'Gradient Boosting Classifier - Randomized Search')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparamos Gradient Boosting Classifier con Random Forest. Para eso, hicimos una búsqueda de parémetro con Randomized Search para los dos algoritmos variando, en el caso de Random Forest n_estimators, max_features y criterion. Para el caso de Gradient Boosting variamos loss,\tmax_features,\tmin_samples_leaf,\tmin_samples_split y n_estimators. La elección de estos hiperparámetros estuvo motivada por usos comunes del clasificador. Ahora bien, a la hora de elegir valores tuvimos en cuenta las características y y el tamaño de nuestros datos. Así, min_sample_leaf y min_sample_split elegimos valores bajos acordes a la cantidad de instancias tenidas en cuenta para el entrenamiento del modelo mientras que para n_estimators el rango elegido fue grande aprovechando la búsqueda random. \n",
    "\n",
    "Como conclusión vemos que Gradient Boosting Classifier rinde mejor que Random Forest. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Competencias\n",
    "\n",
    "La entrega del trabajo estará acompañada de una competencia en la cual deberán poner a prueba su mejor modelo y sobre todo, su capacidad para estimar sus resultados. \n",
    "\n",
    "Su tarea será estimar la performance (AUC ROC) que tendrá su mejor modelo en datos de evaluación (X_competencia). \n",
    "\n",
    "Para ello, deberán predecir las probabilidades de las distintas instancias con su modelo, enviarnos dichas probabilidades junto a una estimación con 4 decimales de cuál será el AUC ROC resultante y calcularemos el resultado real. El grupo que consiga acercarse más al valor real, será el grupo ganador.  \n",
    "\n",
    "Recomendamos no perder de vista esta competencia en el momento de separar los datos en los primeros puntos. \n",
    "\n",
    "Para esto, junto con la entrega del informe, deberán enviar un archivo en formato csv con las columnas “index” y “output” (ver ejemplo de archivo en: [y_competencia_ejemplo.csv](https://github.com/pbrusco/aa-notebooks/blob/master/TP1/y_competencia_ejemplo.csv)) y un valor esperado de AUC ROC. \n",
    "\n",
    "\n",
    "## Entrega\n",
    "- Contarán con un esqueleto en formato Jupyter Notebook en donde tendrán que completar las celdas faltantes (ya sea con explicaciones y gráficos o código). \n",
    "- El notebook final deberá ser entregado en formatos .html e .ipynb. Es necesario que los resultados puedan reproducirse al ejecutar todas las celdas en orden (Kernel - Restart and Run All) utilizando las bibliotecas requeridas en el archivo: requirements.txt del repositorio. \n",
    "- Tienen tiempo hasta las 23:59hs del día miércoles 17/10/2018. La entrega se debe realizar a través del campus virtual y debe contener el informe.\n",
    "- El trabajo deberá elaborarse en grupos de 3 personas.\n",
    "- Se podrán pedir pruebas de integridad y autoría; es decir, verificar que la salida solicitada es fruto del modelo presentado y que el modelo fue construido según lo requerido en este enunciado.\n",
    "- La evaluación será grupal y se basará en la calidad del informe (presentación, claridad, prolijidad); la originalidad, practicidad y coherencia técnica de la solución; la corrección y solidez de las pruebas realizadas.\n",
    "- En el primer parcial se incluirá una pregunta sobre la solución entregada. Esa pregunta no influirá en la nota del parcial, pero sí en la nota individual del TP1.\n",
    "- La participación en la competencia es obligatoria. De todas maneras, el resultado no incidirán en la nota de la materia.\n",
    "- Los ejercicios extra son opcionales para aprobar el TP, pero son obligatorios para promocionar la materia.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = KNeighborsClassifier(n_neighbors = 40, algorithm='kd_tree', weights= 'uniform')\n",
    "clf.fit(X_np, y_np)\n",
    "\n",
    "prediction_eval_c = clf.predict(X_eval)\n",
    "accuracy = accuracy_score(y_eval, prediction_eval_c)\n",
    "prediction_eval_p = clf.predict_proba(X_eval)[:,1]\n",
    "roc = roc_auc_score(y_eval, prediction_eval_p)\n",
    "\n",
    "print('accuracy', accuracy)\n",
    "print('AUC - ROC', roc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finalmente, luego de haber entrenado y testeado varios modelos, con varias combinaciones de hiperparámetros decidimos seleccionar KNN con los siguientes valores:\n",
    "\n",
    "- *n_neighbors = 40*\n",
    "- *algorithm   = kd_tree*\n",
    "- *weights     = uniform*\n",
    "\n",
    "El accuracy de este modelo se encontraba alrededor de **0.78**, tanto para los datos de entrenamiento como para los datos de testeo utilizados durante el desarrollo. Sin embargo, para estimar el valor que se obtendrá al clasificar los datos de la competencia utilizados datos que no fueron utilizados en ninguna otra etapa: los datos held-out.\n",
    "\n",
    "Al entrenar el modelo con todos los datos de desarollo y testearlo con los datos held-out observamos un esperado descenso en el *accuracy*, obteniendo un valor de **0.69**. Por otro lado, el valor de AUC-ROC es de **0.8**, valores que esperamos repetir con los datos predichos para la competencia.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_competencia = clf.predict(X_competencia)\n",
    "y_competencia = pd.DataFrame(data=prediction_competencia, index=X_competencia.index)\n",
    "y_competencia.to_csv('y_competencia.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
