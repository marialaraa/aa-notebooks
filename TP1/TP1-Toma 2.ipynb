{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trabajo Práctico 1 \n",
    "### Clasificación sobre datos simulados. \n",
    "\n",
    "## Introducción\n",
    "Para este trabajo, hemos creado una función generadora de minions. Sobre cada minion, hemos medido 200 características que representan habilidades que poseen en distintas tareas (relacionadas al Mal).  \n",
    "\n",
    "El doctor Nefario ha ideado una fórmula para determinar si un minion es o no apto para concretar su plan para conquistar el mundo. De esta manera ha etiquetado más de 500 minions. Lamentablemente, ha perdido dicha fórmula y necesita seguir decidiendo si nuevos minions son o no aptos para su macabro plan.\n",
    "\n",
    "Es por esto que nuestro objetivo será construir clasificadores que estimen lo mejor posible la probabilidad de que nuevos minions sean o no aptos para concretar el plan de conquista y así facilitarle las cosas al doctor Nefario.\n",
    "\n",
    "Por otra parte, ya que el doctor Nefario tuvo problemas con equipos que sobreestiman sus resultados, decidió guardarse varias etiquetas extra que no compartirá con nadie, y que luego utilizará para elegir al mejor equipo, al cual contratará para (de una vez por todas) conquistar el mundo. \n",
    "\n",
    "\n",
    "En concreto:\n",
    "\n",
    "Tendrán disponible una matriz de datos $X$ de $500$ filas en donde cada fila $x^{(i)}$ representa un vector de $200$ características de cada instancia. Es decir, $\\textbf{x}^{(i)} = x_1^{(i)}, \\dots, x_{200}^{(i)}$ con $i$ entre $1$ y $500$. Además, tendrán y, un vector de $500$ posiciones con dos posibles valores: $True$ y $False$. \n",
    "\n",
    "Por otra parte, tendrán disponibles más instancias de evaluación $X_{competencia}$ sin las respectivas etiquetas que utilizaremos para evaluar sus resultados. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brunobian/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n",
      "/home/brunobian/anaconda3/lib/python3.7/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "# PREAMBULOS\n",
    "%matplotlib inline\n",
    "\n",
    "SEED = 1234\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "from IPython.display import display, HTML\n",
    "from time import time\n",
    "\n",
    "import numpy as np\n",
    "np.set_printoptions(precision=4)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "import pandas as  pd\n",
    "pd.set_option('display.max_rows', 10)\n",
    "pd.set_option('display.max_columns', 15)\n",
    "\n",
    "pd.set_option('precision', 4)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "import sklearn.ensemble\n",
    "import sklearn.naive_bayes\n",
    "import sklearn.svm\n",
    "import sklearn.model_selection\n",
    "\n",
    "from numpy.core.umath_tests import inner1d\n",
    "\n",
    "from scipy.stats import randint as sp_randint\n",
    "\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "\n",
    "\n",
    "from mi_clasificador_arbol import MiClasificadorArbol\n",
    "from learning_curve import plot_learning_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>...</th>\n",
       "      <th>193</th>\n",
       "      <th>194</th>\n",
       "      <th>195</th>\n",
       "      <th>196</th>\n",
       "      <th>197</th>\n",
       "      <th>198</th>\n",
       "      <th>199</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>index</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.4914</td>\n",
       "      <td>0.1644</td>\n",
       "      <td>1.2315</td>\n",
       "      <td>1.2429</td>\n",
       "      <td>1.5576</td>\n",
       "      <td>0.0455</td>\n",
       "      <td>0.1302</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.1983</td>\n",
       "      <td>-0.0118</td>\n",
       "      <td>1.5375</td>\n",
       "      <td>-0.7727</td>\n",
       "      <td>-0.1401</td>\n",
       "      <td>2.0871</td>\n",
       "      <td>-0.8312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.2749</td>\n",
       "      <td>0.2780</td>\n",
       "      <td>-1.3108</td>\n",
       "      <td>0.6801</td>\n",
       "      <td>-0.5503</td>\n",
       "      <td>0.6359</td>\n",
       "      <td>-0.4478</td>\n",
       "      <td>...</td>\n",
       "      <td>1.2190</td>\n",
       "      <td>-0.3190</td>\n",
       "      <td>-0.6446</td>\n",
       "      <td>-0.0061</td>\n",
       "      <td>-1.2374</td>\n",
       "      <td>-1.3291</td>\n",
       "      <td>-1.3265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.2243</td>\n",
       "      <td>-0.5710</td>\n",
       "      <td>-0.2712</td>\n",
       "      <td>-0.1328</td>\n",
       "      <td>-1.0045</td>\n",
       "      <td>0.9315</td>\n",
       "      <td>-1.4507</td>\n",
       "      <td>...</td>\n",
       "      <td>0.9459</td>\n",
       "      <td>0.1430</td>\n",
       "      <td>-0.1989</td>\n",
       "      <td>-0.0393</td>\n",
       "      <td>-0.5866</td>\n",
       "      <td>2.2507</td>\n",
       "      <td>1.4925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.5853</td>\n",
       "      <td>-0.8532</td>\n",
       "      <td>-0.2723</td>\n",
       "      <td>-0.5493</td>\n",
       "      <td>-2.9824</td>\n",
       "      <td>-0.1697</td>\n",
       "      <td>-0.0430</td>\n",
       "      <td>...</td>\n",
       "      <td>1.6488</td>\n",
       "      <td>-0.7363</td>\n",
       "      <td>-0.8866</td>\n",
       "      <td>-1.2717</td>\n",
       "      <td>-0.1493</td>\n",
       "      <td>0.2007</td>\n",
       "      <td>-1.4820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.4155</td>\n",
       "      <td>1.4187</td>\n",
       "      <td>0.6027</td>\n",
       "      <td>-0.7993</td>\n",
       "      <td>0.2939</td>\n",
       "      <td>-0.1796</td>\n",
       "      <td>-0.7140</td>\n",
       "      <td>...</td>\n",
       "      <td>1.1314</td>\n",
       "      <td>-0.4230</td>\n",
       "      <td>-0.2685</td>\n",
       "      <td>0.3045</td>\n",
       "      <td>-1.2245</td>\n",
       "      <td>-1.9421</td>\n",
       "      <td>1.5186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>0.2516</td>\n",
       "      <td>0.9375</td>\n",
       "      <td>-1.1980</td>\n",
       "      <td>0.4577</td>\n",
       "      <td>0.9287</td>\n",
       "      <td>0.5373</td>\n",
       "      <td>0.2476</td>\n",
       "      <td>...</td>\n",
       "      <td>0.5829</td>\n",
       "      <td>-0.5494</td>\n",
       "      <td>0.4607</td>\n",
       "      <td>1.2182</td>\n",
       "      <td>0.1025</td>\n",
       "      <td>3.0034</td>\n",
       "      <td>-0.0344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>0.6246</td>\n",
       "      <td>-1.0590</td>\n",
       "      <td>0.9491</td>\n",
       "      <td>0.2687</td>\n",
       "      <td>0.6610</td>\n",
       "      <td>-1.6657</td>\n",
       "      <td>0.3982</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.1075</td>\n",
       "      <td>0.8993</td>\n",
       "      <td>-0.4229</td>\n",
       "      <td>0.3977</td>\n",
       "      <td>-0.0808</td>\n",
       "      <td>-1.7054</td>\n",
       "      <td>-0.4786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497</th>\n",
       "      <td>0.2677</td>\n",
       "      <td>0.1802</td>\n",
       "      <td>0.7154</td>\n",
       "      <td>0.3542</td>\n",
       "      <td>-0.9023</td>\n",
       "      <td>-1.7792</td>\n",
       "      <td>-0.0121</td>\n",
       "      <td>...</td>\n",
       "      <td>0.8491</td>\n",
       "      <td>0.7469</td>\n",
       "      <td>0.2071</td>\n",
       "      <td>-1.0090</td>\n",
       "      <td>0.3317</td>\n",
       "      <td>-1.7513</td>\n",
       "      <td>-0.5397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>0.1926</td>\n",
       "      <td>0.7834</td>\n",
       "      <td>1.7056</td>\n",
       "      <td>0.3418</td>\n",
       "      <td>-0.8350</td>\n",
       "      <td>0.4068</td>\n",
       "      <td>0.0495</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.0130</td>\n",
       "      <td>0.1483</td>\n",
       "      <td>0.5019</td>\n",
       "      <td>-0.0020</td>\n",
       "      <td>-1.6642</td>\n",
       "      <td>2.5117</td>\n",
       "      <td>-0.0118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>0.0427</td>\n",
       "      <td>0.4028</td>\n",
       "      <td>-0.6085</td>\n",
       "      <td>1.0845</td>\n",
       "      <td>0.1033</td>\n",
       "      <td>0.2698</td>\n",
       "      <td>-0.8598</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.3587</td>\n",
       "      <td>-0.3121</td>\n",
       "      <td>-0.7630</td>\n",
       "      <td>0.6525</td>\n",
       "      <td>0.6161</td>\n",
       "      <td>-0.0902</td>\n",
       "      <td>-1.0215</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>500 rows × 200 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            0       1       2       3       4       5       6   ...       193  \\\n",
       "index                                                           ...             \n",
       "0      1.4914  0.1644  1.2315  1.2429  1.5576  0.0455  0.1302   ...   -1.1983   \n",
       "1     -0.2749  0.2780 -1.3108  0.6801 -0.5503  0.6359 -0.4478   ...    1.2190   \n",
       "2     -0.2243 -0.5710 -0.2712 -0.1328 -1.0045  0.9315 -1.4507   ...    0.9459   \n",
       "3      0.5853 -0.8532 -0.2723 -0.5493 -2.9824 -0.1697 -0.0430   ...    1.6488   \n",
       "4     -1.4155  1.4187  0.6027 -0.7993  0.2939 -0.1796 -0.7140   ...    1.1314   \n",
       "...       ...     ...     ...     ...     ...     ...     ...   ...       ...   \n",
       "495    0.2516  0.9375 -1.1980  0.4577  0.9287  0.5373  0.2476   ...    0.5829   \n",
       "496    0.6246 -1.0590  0.9491  0.2687  0.6610 -1.6657  0.3982   ...   -0.1075   \n",
       "497    0.2677  0.1802  0.7154  0.3542 -0.9023 -1.7792 -0.0121   ...    0.8491   \n",
       "498    0.1926  0.7834  1.7056  0.3418 -0.8350  0.4068  0.0495   ...   -0.0130   \n",
       "499    0.0427  0.4028 -0.6085  1.0845  0.1033  0.2698 -0.8598   ...   -0.3587   \n",
       "\n",
       "          194     195     196     197     198     199  \n",
       "index                                                  \n",
       "0     -0.0118  1.5375 -0.7727 -0.1401  2.0871 -0.8312  \n",
       "1     -0.3190 -0.6446 -0.0061 -1.2374 -1.3291 -1.3265  \n",
       "2      0.1430 -0.1989 -0.0393 -0.5866  2.2507  1.4925  \n",
       "3     -0.7363 -0.8866 -1.2717 -0.1493  0.2007 -1.4820  \n",
       "4     -0.4230 -0.2685  0.3045 -1.2245 -1.9421  1.5186  \n",
       "...       ...     ...     ...     ...     ...     ...  \n",
       "495   -0.5494  0.4607  1.2182  0.1025  3.0034 -0.0344  \n",
       "496    0.8993 -0.4229  0.3977 -0.0808 -1.7054 -0.4786  \n",
       "497    0.7469  0.2071 -1.0090  0.3317 -1.7513 -0.5397  \n",
       "498    0.1483  0.5019 -0.0020 -1.6642  2.5117 -0.0118  \n",
       "499   -0.3121 -0.7630  0.6525  0.6161 -0.0902 -1.0215  \n",
       "\n",
       "[500 rows x 200 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>output</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>index</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>500 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       output\n",
       "index        \n",
       "0           0\n",
       "1           0\n",
       "2           0\n",
       "3           0\n",
       "4           1\n",
       "...       ...\n",
       "495         1\n",
       "496         0\n",
       "497         1\n",
       "498         0\n",
       "499         0\n",
       "\n",
       "[500 rows x 1 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>...</th>\n",
       "      <th>193</th>\n",
       "      <th>194</th>\n",
       "      <th>195</th>\n",
       "      <th>196</th>\n",
       "      <th>197</th>\n",
       "      <th>198</th>\n",
       "      <th>199</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>500.0000</td>\n",
       "      <td>500.0000</td>\n",
       "      <td>500.0000</td>\n",
       "      <td>500.0000</td>\n",
       "      <td>500.0000</td>\n",
       "      <td>500.0000</td>\n",
       "      <td>500.0000</td>\n",
       "      <td>...</td>\n",
       "      <td>500.0000</td>\n",
       "      <td>500.0000</td>\n",
       "      <td>500.0000</td>\n",
       "      <td>500.0000</td>\n",
       "      <td>500.0000</td>\n",
       "      <td>500.0000</td>\n",
       "      <td>500.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.0384</td>\n",
       "      <td>0.0715</td>\n",
       "      <td>0.0056</td>\n",
       "      <td>-0.0103</td>\n",
       "      <td>-0.0436</td>\n",
       "      <td>-0.0208</td>\n",
       "      <td>-0.0571</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.0187</td>\n",
       "      <td>0.0087</td>\n",
       "      <td>-0.0356</td>\n",
       "      <td>-0.1940</td>\n",
       "      <td>0.0250</td>\n",
       "      <td>0.0257</td>\n",
       "      <td>-0.0036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.0153</td>\n",
       "      <td>0.9613</td>\n",
       "      <td>1.0360</td>\n",
       "      <td>1.0230</td>\n",
       "      <td>1.0647</td>\n",
       "      <td>0.9898</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.9731</td>\n",
       "      <td>0.9716</td>\n",
       "      <td>1.0075</td>\n",
       "      <td>1.0246</td>\n",
       "      <td>0.9934</td>\n",
       "      <td>0.9940</td>\n",
       "      <td>0.9819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-3.1722</td>\n",
       "      <td>-2.4596</td>\n",
       "      <td>-2.8834</td>\n",
       "      <td>-3.7474</td>\n",
       "      <td>-2.9987</td>\n",
       "      <td>-3.2014</td>\n",
       "      <td>-3.6855</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.9110</td>\n",
       "      <td>-2.9642</td>\n",
       "      <td>-2.5163</td>\n",
       "      <td>-3.9278</td>\n",
       "      <td>-2.4254</td>\n",
       "      <td>-2.6234</td>\n",
       "      <td>-2.8690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-0.6090</td>\n",
       "      <td>-0.6207</td>\n",
       "      <td>-0.7180</td>\n",
       "      <td>-0.6594</td>\n",
       "      <td>-0.7177</td>\n",
       "      <td>-0.6510</td>\n",
       "      <td>-0.7073</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.6441</td>\n",
       "      <td>-0.6509</td>\n",
       "      <td>-0.6613</td>\n",
       "      <td>-0.8689</td>\n",
       "      <td>-0.6466</td>\n",
       "      <td>-0.6354</td>\n",
       "      <td>-0.6855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.0602</td>\n",
       "      <td>0.0560</td>\n",
       "      <td>-0.0713</td>\n",
       "      <td>0.0612</td>\n",
       "      <td>-0.0097</td>\n",
       "      <td>-0.0407</td>\n",
       "      <td>-0.0771</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.0473</td>\n",
       "      <td>0.0537</td>\n",
       "      <td>-0.0749</td>\n",
       "      <td>-0.1901</td>\n",
       "      <td>0.0185</td>\n",
       "      <td>-0.0332</td>\n",
       "      <td>-0.0797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.6334</td>\n",
       "      <td>0.7670</td>\n",
       "      <td>0.7066</td>\n",
       "      <td>0.6699</td>\n",
       "      <td>0.6616</td>\n",
       "      <td>0.6508</td>\n",
       "      <td>0.6029</td>\n",
       "      <td>...</td>\n",
       "      <td>0.6071</td>\n",
       "      <td>0.6860</td>\n",
       "      <td>0.5743</td>\n",
       "      <td>0.4636</td>\n",
       "      <td>0.7041</td>\n",
       "      <td>0.6575</td>\n",
       "      <td>0.6608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2.9702</td>\n",
       "      <td>2.7920</td>\n",
       "      <td>2.6905</td>\n",
       "      <td>2.8091</td>\n",
       "      <td>2.9823</td>\n",
       "      <td>2.9342</td>\n",
       "      <td>3.3240</td>\n",
       "      <td>...</td>\n",
       "      <td>3.0983</td>\n",
       "      <td>3.1469</td>\n",
       "      <td>2.9109</td>\n",
       "      <td>2.4942</td>\n",
       "      <td>3.1804</td>\n",
       "      <td>3.0034</td>\n",
       "      <td>2.5107</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 200 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              0         1         2         3         4         5         6  \\\n",
       "count  500.0000  500.0000  500.0000  500.0000  500.0000  500.0000  500.0000   \n",
       "mean     0.0384    0.0715    0.0056   -0.0103   -0.0436   -0.0208   -0.0571   \n",
       "std      1.0153    0.9613    1.0360    1.0230    1.0647    0.9898    1.0000   \n",
       "min     -3.1722   -2.4596   -2.8834   -3.7474   -2.9987   -3.2014   -3.6855   \n",
       "25%     -0.6090   -0.6207   -0.7180   -0.6594   -0.7177   -0.6510   -0.7073   \n",
       "50%      0.0602    0.0560   -0.0713    0.0612   -0.0097   -0.0407   -0.0771   \n",
       "75%      0.6334    0.7670    0.7066    0.6699    0.6616    0.6508    0.6029   \n",
       "max      2.9702    2.7920    2.6905    2.8091    2.9823    2.9342    3.3240   \n",
       "\n",
       "         ...          193       194       195       196       197       198  \\\n",
       "count    ...     500.0000  500.0000  500.0000  500.0000  500.0000  500.0000   \n",
       "mean     ...      -0.0187    0.0087   -0.0356   -0.1940    0.0250    0.0257   \n",
       "std      ...       0.9731    0.9716    1.0075    1.0246    0.9934    0.9940   \n",
       "min      ...      -2.9110   -2.9642   -2.5163   -3.9278   -2.4254   -2.6234   \n",
       "25%      ...      -0.6441   -0.6509   -0.6613   -0.8689   -0.6466   -0.6354   \n",
       "50%      ...      -0.0473    0.0537   -0.0749   -0.1901    0.0185   -0.0332   \n",
       "75%      ...       0.6071    0.6860    0.5743    0.4636    0.7041    0.6575   \n",
       "max      ...       3.0983    3.1469    2.9109    2.4942    3.1804    3.0034   \n",
       "\n",
       "            199  \n",
       "count  500.0000  \n",
       "mean    -0.0036  \n",
       "std      0.9819  \n",
       "min     -2.8690  \n",
       "25%     -0.6855  \n",
       "50%     -0.0797  \n",
       "75%      0.6608  \n",
       "max      2.5107  \n",
       "\n",
       "[8 rows x 200 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Carga de datos\n",
    "X = pd.read_csv(\"../TP1/X.csv\", index_col=\"index\")\n",
    "y = pd.read_csv(\"../TP1/y.csv\", index_col=\"index\", dtype=int)  # Cargamos los valores booleanos (True y False)\n",
    "                                                        # como números (1 y 0) para facilitar el manejo luego. \n",
    "    \n",
    "X_competencia = pd.read_csv(\"../TP1/X_competencia1.csv\", index_col=\"index\")\n",
    "y_competencia_ejemplo = pd.read_csv(\"../TP1/y_competencia_ejemplo.csv\", index_col=\"index\")\n",
    "display(X)\n",
    "display(y)\n",
    "X.describe()\n",
    "\n",
    "# Descomentar si quieren ver los datos para la competencia:\n",
    "# display(X_competencia) \n",
    "# display(y_competencia_ejemplo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejercicio 1\n",
    "\n",
    "### Separación de datos\n",
    "\n",
    "Contarán con una cantidad limitada de datos, por lo cual es importante tomar una buena decisión en el momento de empezar a utilizarlos. En este punto pedimos que evalúen cómo separar sus datos para desarrollo y para evaluación tomando en cuenta la competencia. No consideramos necesario normalizar los datos. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Del total de los datos nos quedamos con un 0.2% como held out que no fue utilizado a lo largo del trabajo práctico. A la hora de la separación tuvimos en cuenta la distribución de los datos para que fuese equivalente en ambos grupos. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_dev: (400, 200), y_dev: (400, 1) para desarrollo\n",
      "X_eval: (100, 200), y_eval: (100, 1) para evaluación\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x216 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "########################################################\n",
    "\n",
    "X_dev, X_eval, y_dev, y_eval = sklearn.model_selection.train_test_split(X, y, random_state=SEED, test_size=0.2)\n",
    "\n",
    "#########################################################\n",
    "\n",
    "print(\"X_dev: {}, y_dev: {} para desarrollo\".format(X_dev.shape, y_dev.shape))\n",
    "print(\"X_eval: {}, y_eval: {} para evaluación\".format(X_eval.shape, y_eval.shape))\n",
    "\n",
    "plt.figure(figsize=(5, 3))\n",
    "plt.hist(np.array(y_dev))  # muestra un histograma para la distribución de y.\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejercicio 2\n",
    "\n",
    "### Construcción de modelos\n",
    "\n",
    "Para este punto, la tarea consiste en construir y evaluar modelos de tipo árbol de decisión, de manera de obtener una estimación realista de la performance de los mismos. \n",
    "\n",
    "1. Entrenar un árbol de decisión con altura máxima 3 y el resto de los hiperparámetros en default. \n",
    "2. Estimar la performance del modelo utilizando K-fold cross validation con K = 5, con las métricas “Accuracy” y “ROC AUC”. Para ello, se pide medir la performance en cada partición tanto sobre el fold de validación como sobre los folds de entrenamiento. Luego, completar la primera tabla.\n",
    "3. Entrenar árboles de decisión para cada una de las siguientes combinaciones y completar la segunda tabla.\n",
    "\n",
    "----\n",
    "\n",
    "**EJERCICIO EXTRA**: Usar la implementación de árboles de decisión que realizaron para la guía de ejercicios de la materia. Adaptarla para que cumpla con la interfaz requerida por sklearn, asegurarse de que funcione con variables continuas y reproducir las tablas anteriores.   \n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_dev_np = np.array(X_dev)\n",
    "y_dev_np = np.array(y_dev).ravel()\n",
    "\n",
    "kf = KFold(n_splits=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Aclaración**: en este punto dividiremos el *X_dev*, definido inicialmente resultado de la separación del **held-out**, en dos sets: uno llamado *train*, con el que entrenaremos los modelos; y otro llamado *test*, con el que validaremos los modelos (*cross-validation*). Por otro lado, en los casos en los que usamos *k-fold cross-validation* seleccionaremos los *folds* sobre *X_dev*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<h3> TABLA 1 </h3>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy (train)</th>\n",
       "      <th>Accuracy (val)</th>\n",
       "      <th>AUC ROC (train)</th>\n",
       "      <th>AUC ROC (val)</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Perm</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.8031</td>\n",
       "      <td>0.6000</td>\n",
       "      <td>0.8845</td>\n",
       "      <td>0.6006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.8500</td>\n",
       "      <td>0.6250</td>\n",
       "      <td>0.8813</td>\n",
       "      <td>0.6512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.8187</td>\n",
       "      <td>0.6125</td>\n",
       "      <td>0.8810</td>\n",
       "      <td>0.5918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.8281</td>\n",
       "      <td>0.7250</td>\n",
       "      <td>0.8618</td>\n",
       "      <td>0.7332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.8094</td>\n",
       "      <td>0.5750</td>\n",
       "      <td>0.8656</td>\n",
       "      <td>0.5969</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Accuracy (train)  Accuracy (val)  AUC ROC (train)  AUC ROC (val)\n",
       "Perm                                                                  \n",
       "1               0.8031          0.6000           0.8845         0.6006\n",
       "2               0.8500          0.6250           0.8813         0.6512\n",
       "3               0.8187          0.6125           0.8810         0.5918\n",
       "4               0.8281          0.7250           0.8618         0.7332\n",
       "5               0.8094          0.5750           0.8656         0.5969"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "accuracies_training, accuracies_validation, aucs_training, aucs_validation = [], [], [], []\n",
    "\n",
    "########################################################\n",
    "# (1) y (2)\n",
    "########################################################\n",
    "for dev_index, eval_index in kf.split(X_dev_np):\n",
    "    X_dev_folds, X_eval_folds = X_dev_np[dev_index], X_dev_np[eval_index]\n",
    "    y_dev_folds, y_eval_folds = y_dev_np[dev_index], y_dev_np[eval_index]\n",
    "    \n",
    "    tree = DecisionTreeClassifier(max_depth=3)\n",
    "    tree.fit(X_dev_folds, y_dev_folds)\n",
    "    \n",
    "    prediction_dev_c  = tree.predict(X_dev_folds) # _c es para cuando devuelve clase\n",
    "    prediction_eval_c = tree.predict(X_eval_folds)\n",
    "    \n",
    "    accuracies_training.append(accuracy_score(y_dev_folds, prediction_dev_c))\n",
    "    accuracies_validation.append(accuracy_score(y_eval_folds, prediction_eval_c))\n",
    "\n",
    "    prediction_dev_p  = tree.predict_proba(X_dev_folds)[:,1] # _p es para cuando devuelve probabilidades\n",
    "    prediction_eval_p = tree.predict_proba(X_eval_folds)[:,1]\n",
    "        \n",
    "    aucs_training.append(roc_auc_score(y_dev_folds, prediction_dev_p))\n",
    "    aucs_validation.append(roc_auc_score(y_eval_folds, prediction_eval_p))\n",
    "    \n",
    "#########################################################\n",
    "\n",
    "df = pd.DataFrame(index=range(1,6))\n",
    "df.index.name = \"Perm\"\n",
    "                  \n",
    "df[\"Accuracy (train)\"]   = accuracies_training   \n",
    "df[\"Accuracy (val)\"] = accuracies_validation  \n",
    "df[\"AUC ROC (train)\"]    = aucs_training      \n",
    "df[\"AUC ROC (val)\"]  = aucs_validation    \n",
    "\n",
    "\n",
    "display(HTML(\"<h3> TABLA 1 </h3>\"))\n",
    "display(df)\n",
    "\n",
    "df.plot(kind=\"bar\")\n",
    "plt.legend(loc='upper left', bbox_to_anchor=(1.0, 1.0))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como se puede observar, tanto los valores de Accuracy como los de AUC-ROC se mantienen estables entre los folds, tanto para el entrenamiento (train) como para la validación (val). Además, se observa una esperada baja en ambas métricas cuando pasamos de predecir los datos de entrenamiento a predecir los de validación. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(X_dev, y_dev, random_state=SEED, test_size=0.2)\n",
    "\n",
    "X_train_np = np.array(X_train)\n",
    "y_train_np = np.array(y_train).ravel()\n",
    "X_test_np  = np.array(X_test)\n",
    "y_test_np  = np.array(y_test).ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<h3> TABLA 2 </h3>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Altura máxima</th>\n",
       "      <th>Criterio de evaluación de corte</th>\n",
       "      <th>AUC ROC promedio (training)</th>\n",
       "      <th>AUC ROC promedio (validación)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>Gini</td>\n",
       "      <td>0.8832</td>\n",
       "      <td>0.6934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>Gini</td>\n",
       "      <td>0.9666</td>\n",
       "      <td>0.7201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Inifinito</td>\n",
       "      <td>Gini</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.6562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Ganancia de Información</td>\n",
       "      <td>0.8869</td>\n",
       "      <td>0.6888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Ganancia de Información</td>\n",
       "      <td>0.9865</td>\n",
       "      <td>0.6729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Inifinito</td>\n",
       "      <td>Ganancia de Información</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.6250</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Altura máxima Criterio de evaluación de corte  AUC ROC promedio (training)  \\\n",
       "0             3                            Gini                       0.8832   \n",
       "1             5                            Gini                       0.9666   \n",
       "2     Inifinito                            Gini                       1.0000   \n",
       "3             3         Ganancia de Información                       0.8869   \n",
       "4             5         Ganancia de Información                       0.9865   \n",
       "5     Inifinito         Ganancia de Información                       1.0000   \n",
       "\n",
       "   AUC ROC promedio (validación)  \n",
       "0                         0.6934  \n",
       "1                         0.7201  \n",
       "2                         0.6562  \n",
       "3                         0.6888  \n",
       "4                         0.6729  \n",
       "5                         0.6250  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "########################################################\n",
    "# (3)\n",
    "########################################################\n",
    "\n",
    "AUC_train, AUC_val = [], []\n",
    "\n",
    "for criterio in [\"gini\", \"entropy\"]:\n",
    "     for altura in [3, 5, None]:\n",
    "        tree = DecisionTreeClassifier(max_depth=altura, criterion=criterio, random_state=SEED)\n",
    "        tree.fit(X_train, y_train)\n",
    "        \n",
    "        pred_dev_p  = tree.predict_proba(X_train)[:,1]\n",
    "        pred_eval_p = tree.predict_proba(X_test)[:,1]\n",
    "        \n",
    "        AUC_train.append(roc_auc_score(y_train, pred_dev_p))\n",
    "        AUC_val.append(roc_auc_score(y_test, pred_eval_p))\n",
    "\n",
    "df = pd.DataFrame(index=range(0,6))\n",
    "\n",
    "df[\"Altura máxima\"] = [3, 5, \"Inifinito\"] * 2\n",
    "df[\"Criterio de evaluación de corte\"] = [\"Gini\"] * 3 + [\"Ganancia de Información\"] * 3\n",
    "df[\"AUC ROC promedio (training)\"] = AUC_train # reemplazar por resultados_training\n",
    "df[\"AUC ROC promedio (validación)\"] = AUC_val # reemplazar por resultados_validation\n",
    "\n",
    "   \n",
    "display(HTML(\"<h3> TABLA 2 </h3>\"))\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nuevamente, al igual en en el punto anterior, se observa una mejor predicción sobre los datos vistos en el entrenamiento con respecto a los datos no vistos en el entrenamiento. Con respecto a los parámetros variados (altura máxima y criterio de evaluación de corte) podemos ver que: (1) la profundidad del árbol es determinante para el desempeño de nuestra clasificación, llegando incluso a modelos overfitteados cuando permitimos demasiada flexibilidad (mayor profundidad) a los mismos; y (2) no existe mucha diferencia entre utilizar Gini o Information Gain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Extra] Mi Clasificador Árbol\n",
    "\n",
    "# ACA HAY QUE HACER COSAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracies_training, accuracies_validation, aucs_training, aucs_validation = [], [], [], []\n",
    "\n",
    "for dev_index, eval_index in kf.split(X_dev_np):\n",
    "    X_dev_folds  = X_dev_np[dev_index]\n",
    "    X_eval_folds = X_dev_np[eval_index]\n",
    "    y_dev_folds, y_eval_folds = y_dev_np[dev_index], y_dev_np[eval_index]\n",
    "    \n",
    "    tree = MiClasificadorArbol(list(X.columns), profundidad_max=3, criterion='gini')\n",
    "    tree.fit(X_dev_folds, y_dev_folds)\n",
    "    \n",
    "    prediction_dev = tree.predict(X_dev_folds)\n",
    "    prediction_eval = tree.predict(X_eval_folds)\n",
    "    \n",
    "    accuracies_training.append(accuracy_score(y_dev_folds, prediction_dev))\n",
    "    accuracies_validation.append(accuracy_score(y_eval_folds, prediction_eval))\n",
    "    \n",
    "    ####### PARA ESTO HAY QUE IMPLEMENTAR UN predict_proba\n",
    "    aucs_training.append(roc_auc_score(y_dev_folds, prediction_dev))\n",
    "    aucs_validation.append(roc_auc_score(y_eval_folds, prediction_eval))\n",
    "    \n",
    "## Objetivo: accuracies_training, accuracies_validation, aucs_training y aucs_validation asignados\n",
    "#########################################################\n",
    "\n",
    "df = pd.DataFrame(index=range(1,6))\n",
    "df.index.name = \"Permutación\"\n",
    "                  \n",
    "df[\"Accuracy (training)\"] = accuracies_training   \n",
    "df[\"Accuracy (validación)\"] = accuracies_validation  \n",
    "df[\"AUC ROC (training)\"] = aucs_training      \n",
    "df[\"AUC ROC (validación)\"] = aucs_validation    \n",
    "\n",
    "\n",
    "display(HTML(\"<h3> TABLA 1 </h3>\"))\n",
    "display(df)\n",
    "\n",
    "# Descomentar las siguientes líneas para graficar el resultado\n",
    "df.plot(kind=\"bar\")\n",
    "plt.legend(loc='upper left', bbox_to_anchor=(1.0, 1.0))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################\n",
    "# (EE 3)\n",
    "########################################################\n",
    "\n",
    "resultados_training = []\n",
    "resultados_validation = []\n",
    "\n",
    "for criterio in [\"gini\", \"entropy\"]:\n",
    "    for altura in [3, 5, None]:\n",
    "        tree = MiClasificadorArbol(list(X.columns), profundidad_max=altura, criterion=criterio)\n",
    "        tree.fit(X_train_np, y_train_np)\n",
    "        \n",
    "        prediction_dev  = tree.predict(X_train_np)\n",
    "        prediction_eval = tree.predict(X_test_np)\n",
    "\n",
    "        resultados_training.append(roc_auc_score(y_train, prediction_dev))\n",
    "        resultados_validation.append(roc_auc_score(y_test, prediction_eval))\n",
    "\n",
    "df = pd.DataFrame(index=range(0,6))\n",
    "\n",
    "df[\"Altura máxima\"] = [3, 5, \"Inifinito\"] * 2\n",
    "df[\"Criterio de evaluación de corte\"] = [\"Gini\"] * 3 + [\"Ganancia de Información\"] * 3\n",
    "df[\"AUC ROC promedio (training)\"] = resultados_training # reemplazar por resultados_training\n",
    "df[\"AUC ROC promedio (validación)\"] = resultados_validation # reemplazar por resultados_validation\n",
    "\n",
    "   \n",
    "display(HTML(\"<h3> TABLA 2 </h3>\"))\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejercicio 3: Comparación de algoritmos\n",
    "\n",
    "\n",
    "Se pide explorar distintas combinaciones de algoritmos de aprendizaje e hiperparámetros, de manera de buscar una performance óptima. Para este ejercicio es necesario que evalúen posibilidades utilizando la técnica de Grid Search. Como métrica de performance, usar siempre el área bajo la curva (AUC ROC) resultante de 5-fold cross-validation. \n",
    "\n",
    "Algoritmos a probar: KNN, árboles de decisión, LDA, Naive Bayes y SVM. Hiperparámetros: Revisar la documentación de cada uno para la búsqueda de combinaciones prometedoras.  \n",
    "\n",
    "Se pide generar un reporte que contenga: \n",
    "\n",
    "1. Una descripción de las distintas combinaciones consideradas y su performance asociada (las que consideren relevantes, con al menos la mejor combinación para cada algoritmo). \n",
    "\n",
    "1. Una breve explicación de los factores que creen que produjeron dicho resultado. \n",
    "\n",
    "En este punto evaluaremos tanto los hiperparámetros elegidos como las conclusiones relacionadas a por qué piensan que ciertos algoritmos funcionan mejor que otros para estos datos. \n",
    "\n",
    "\n",
    "\n",
    "----\n",
    "\n",
    "**EJERCICIO EXTRA**: Utilizar RandomizedSearchCV con rangos de parámetros que contengan a los utilizados en el GridSearch. Analizar si se encontraron mejores combinaciones de parámetros que no hayan sido tenidas en cuenta con el GridSearch y cuál fue la diferencia de tiempo de ejecución. \n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# En todo este ejercicio no usamos la funcion auc_roc, los scores los tiene adentro el objeto que crea el clasificador, supongo que está bien calculado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_resultados(grid, top=5, algorithm_name=''):\n",
    "    ## Si quieren, pueden utilizar esta función para imprimir las mejores combinaciones de su grid\n",
    "    print(algorithm_name)\n",
    "    print(\"Top {} combinaciones\".format(top))\n",
    "    df = pd.DataFrame(grid.cv_results_[\"params\"])\n",
    "    df[\"mean_score_validation\"] = grid.cv_results_[\"mean_test_score\"]\n",
    "    df[\"mean_score_training\"] = grid.cv_results_[\"mean_train_score\"]\n",
    "    display(df.sort_values(by=\"mean_score_validation\", ascending=False).head(top))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_np = np.array(X_dev)\n",
    "y_np = np.array(y_dev).ravel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNeighbors Classifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNeighbors Classifier - Grid Search\n",
      "Top 10 combinaciones\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>algorithm</th>\n",
       "      <th>n_neighbors</th>\n",
       "      <th>mean_score_validation</th>\n",
       "      <th>mean_score_training</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ball_tree</td>\n",
       "      <td>70</td>\n",
       "      <td>0.7725</td>\n",
       "      <td>0.7738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>auto</td>\n",
       "      <td>70</td>\n",
       "      <td>0.7725</td>\n",
       "      <td>0.7738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>kd_tree</td>\n",
       "      <td>70</td>\n",
       "      <td>0.7725</td>\n",
       "      <td>0.7738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>brute</td>\n",
       "      <td>70</td>\n",
       "      <td>0.7725</td>\n",
       "      <td>0.7738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ball_tree</td>\n",
       "      <td>60</td>\n",
       "      <td>0.7700</td>\n",
       "      <td>0.7794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>auto</td>\n",
       "      <td>60</td>\n",
       "      <td>0.7700</td>\n",
       "      <td>0.7794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>kd_tree</td>\n",
       "      <td>60</td>\n",
       "      <td>0.7700</td>\n",
       "      <td>0.7794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>brute</td>\n",
       "      <td>60</td>\n",
       "      <td>0.7700</td>\n",
       "      <td>0.7794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ball_tree</td>\n",
       "      <td>50</td>\n",
       "      <td>0.7625</td>\n",
       "      <td>0.7800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>auto</td>\n",
       "      <td>50</td>\n",
       "      <td>0.7625</td>\n",
       "      <td>0.7800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    algorithm  n_neighbors  mean_score_validation  mean_score_training\n",
       "2   ball_tree           70                 0.7725               0.7738\n",
       "17       auto           70                 0.7725               0.7738\n",
       "7     kd_tree           70                 0.7725               0.7738\n",
       "12      brute           70                 0.7725               0.7738\n",
       "1   ball_tree           60                 0.7700               0.7794\n",
       "16       auto           60                 0.7700               0.7794\n",
       "6     kd_tree           60                 0.7700               0.7794\n",
       "11      brute           60                 0.7700               0.7794\n",
       "0   ball_tree           50                 0.7625               0.7800\n",
       "15       auto           50                 0.7625               0.7800"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNeighbors Classifier - Randomized Search\n",
      "Top 10 combinaciones\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>algorithm</th>\n",
       "      <th>n_neighbors</th>\n",
       "      <th>mean_score_validation</th>\n",
       "      <th>mean_score_training</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>brute</td>\n",
       "      <td>67</td>\n",
       "      <td>0.7775</td>\n",
       "      <td>0.7756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>kd_tree</td>\n",
       "      <td>59</td>\n",
       "      <td>0.7750</td>\n",
       "      <td>0.7788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>kd_tree</td>\n",
       "      <td>64</td>\n",
       "      <td>0.7750</td>\n",
       "      <td>0.7769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>kd_tree</td>\n",
       "      <td>53</td>\n",
       "      <td>0.7675</td>\n",
       "      <td>0.7794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>auto</td>\n",
       "      <td>83</td>\n",
       "      <td>0.7600</td>\n",
       "      <td>0.7638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ball_tree</td>\n",
       "      <td>89</td>\n",
       "      <td>0.7550</td>\n",
       "      <td>0.7606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>kd_tree</td>\n",
       "      <td>89</td>\n",
       "      <td>0.7550</td>\n",
       "      <td>0.7606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>auto</td>\n",
       "      <td>88</td>\n",
       "      <td>0.7550</td>\n",
       "      <td>0.7644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>auto</td>\n",
       "      <td>77</td>\n",
       "      <td>0.7525</td>\n",
       "      <td>0.7663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ball_tree</td>\n",
       "      <td>81</td>\n",
       "      <td>0.7475</td>\n",
       "      <td>0.7650</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   algorithm  n_neighbors  mean_score_validation  mean_score_training\n",
       "6      brute           67                 0.7775               0.7756\n",
       "3    kd_tree           59                 0.7750               0.7788\n",
       "9    kd_tree           64                 0.7750               0.7769\n",
       "2    kd_tree           53                 0.7675               0.7794\n",
       "8       auto           83                 0.7600               0.7638\n",
       "4  ball_tree           89                 0.7550               0.7606\n",
       "5    kd_tree           89                 0.7550               0.7606\n",
       "7       auto           88                 0.7550               0.7644\n",
       "1       auto           77                 0.7525               0.7663\n",
       "0  ball_tree           81                 0.7475               0.7650"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "parameters_knn = {\n",
    "    'n_neighbors': [50, 60, 70, 80, 90], \n",
    "    'algorithm': ['ball_tree', 'kd_tree', 'brute', 'auto']\n",
    "}\n",
    "clf = GridSearchCV(KNeighborsClassifier(weights='uniform'), parameters_knn, cv=5)\n",
    "clf.fit(X_np, y_np)\n",
    "top_resultados(clf, 10, 'KNeighbors Classifier - Grid Search')\n",
    "\n",
    "parameters_knn = {\n",
    "    'n_neighbors': list(range(50, 90)), \n",
    "    'algorithm': ['ball_tree', 'kd_tree', 'brute', 'auto']\n",
    "}\n",
    "n_iter = 10\n",
    "clf = RandomizedSearchCV(KNeighborsClassifier(weights='uniform'), parameters_knn, cv=5, n_iter=n_iter)\n",
    "clf.fit(X_np, y_np)\n",
    "top_resultados(clf, n_iter, 'KNeighbors Classifier - Randomized Search')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Del total de 8 parametros probamos todas las permutaciones de los siguientes tres hiperparámetros para distintos valores:\n",
    "\n",
    "- *n_neighbors*: número de vecinos utilizado por kneighbors query. Que por default es 5. \n",
    "- *weights*: es la función de peso que se utiliza en la predicción. Tiene como opciones: uniform (todos los puntos del vecindario pesan lo mismo), distance (los vecinos más cercanos tienen más peso) y callable (función definida por el usuario). \n",
    "- *algorithm*: es el algoritmo utilizado para computar los vecinos más cercanos. Puede ser: ball_tree, kd_tree, brute o \n",
    "auto. \n",
    "\n",
    "En una primera corrida los valores utilizados para *n_neighbors* fueron 1, 5, 10 y 50.\n",
    "De las opciones de *weights* se dejó afuera *callable* por falta de experiencia.\n",
    "De *algorithm* se corrieron las opciones *ball_tree, kd_tree, brute y auto*.\n",
    "\n",
    "Respecto de la performance. Los mejores rendimientos están dados por las combinaciones: auto/brute/kd_tree, 50, uniform. En todas ellas el accuracy promedio sobre los datos de validación es de alrededor de **0.77**, mientras que el accuracy promedio sobre los datos de entrenamiento es cercano a **0.78**. En las otras combinaciones el primer valor baja o se mantiene, mientras que el accuracy del training llega a 1, clara muestra de *overfitting*.\n",
    "\n",
    "Pareciera que lo que marca la diferencia es el *weight*. Siempre mejora con uniform mientras que overfittea con distance bajo las mismas combinaciones de los otros parametros. \n",
    "\n",
    "A partir de esos resultados decidimos seguir explorando combinaciones: variamos los valores de *vecinos* entre 50 y 100, mantenemos las opciones de *weights* y fijamos *algorithm* en *uniform*.\n",
    "\n",
    "Vemos entonces que la mejor combinación de hiperparámetros obtenida corresponde a 70 vecinos con un accuracy promedio para los datos de validación de **0.7725**. La cantidad de vecinos representa en el algoritmo la cantidad de elementos que van a votar para clasificar el nuevo valor. \n",
    "\n",
    "Finalmente, se buscó optimizar la búsqueda de los hiperparámetros a través de Randomized Search. Para esto se definió un rango de valores de vecinos continuo entre los extremos utilizados en la búsqueda anterior (50 y 100). Como resultado se vió que la combinación de hiperparámetros para este modelo es con *algorith* igual a *auto*, la cantidad de vecinos 69 y *weigths* igual a *uniform*, con valores de *accuracy* similares a lo expresado previamente. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Discriminant Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "clf = GridSearchCV(LinearDiscriminantAnalysis(), {}, cv=5)\n",
    "clf.fit(X_np, y_np)\n",
    "top_resultados(clf, 1, 'Linear Discriminant Analysis - Grid Search')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De acuerdo a lo visto en la teórica y a la documentación de sklearn este modelo no presenta hiperparámetros para tunear. Por lo tanto mostramos un único resultado utilizando todas las variables por default. A partir de haber obtenido un valor de *accuracy* cercano a 1 para los datos de entrenamiento, pero un valor cercano al *random* en la favlidación, concluímos que este modelo está overfitteando."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters_tree = {\n",
    "    'criterion': ('gini', 'entropy'), \n",
    "    'splitter': ('random', 'best'), \n",
    "    'max_depth': list(range(1, 11)) + [50, 100, 150, None]\n",
    "}\n",
    "clf = GridSearchCV(DecisionTreeClassifier(), parameters_tree, cv=5)\n",
    "clf.fit(X_np, y_np)\n",
    "top_resultados(clf, 10, 'Decision Tree Classifier - Grid Search')\n",
    "\n",
    "parameters_tree = {\n",
    "    'criterion': ('gini', 'entropy'), \n",
    "    'splitter': ('random', 'best'), \n",
    "    'max_depth': list(range(1, 30))\n",
    "}\n",
    "n_iter = 100\n",
    "clf = RandomizedSearchCV(DecisionTreeClassifier(), parameters_tree, n_iter, cv=5)\n",
    "clf.fit(X_np, y_np)\n",
    "top_resultados(clf, n_iter, 'Decision Tree Classifier - Randomized Search')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Del total de 13 parámetros probamos todas las permutaciones de los siguientes tres hiperparámetros para distintos valores:\n",
    "- *criterion*: es la función que mide la calidad del split. Las opciones son gini y entropy. \n",
    "- *splitter*: es la estrategia utilizada para elegir la partición en cada nodo. Puede ser best, para elegir el mejor corte o random que elige el mejor corte al azar. \n",
    "- *max_depth*: es la profundidad máxima del árbol. Puede ser seteada como int o como None. Si es None, los nodos se expanden hasta que todas las hojas son puras o hasta que contengan menos muestras que min_samples_split.\n",
    "\n",
    "En una primera corrida los valores utilizados para *criterion* fueron *entropy* y *gini*, mientras que para *splitter* fueron *random* y *best*. Para el hiperparámetro *max_depth* utilizamos 3, 50, 100 y None. \n",
    "\n",
    "Respecto de la performance, los mejores rendimientos estuvieron dados por las combinaciones gini/random/3 con un accuracy promedio para los datos de validación cercanos a **0.69** y con un accuracy promedio para los datos de entrenamiento cercanos a **0.77**, quedando en segundo lugar la combinación entropy/best/3 con un *mean_score_validation* cercano a **0.67** de y con un *mean_score_training* cercano a **0.8**. Para todos los demás casos, las cominaciones resultaron en un *mean_score_training* de **1.00**, es decir, estos modelos están overfitteando sobre los datos de entrenamiento. La razón parece ser, sobre todo, los valores altos de max_depth en relación a la cantidad de atributos. \n",
    "\n",
    "Con lo anterior en mente nos dispusimos a variar la profundidad de los árboles. Para una segunda corrida utilizamos un rango continuo de 1 al 10 y los valores 50, 100 y None. Podemos ver que los mejores modelos obtenidos tienen siempre una profundidad entre 2 y 4. No vemos que exista un sólo valor óptimo, sino que el hiperparámetro *splitter* en *random* hace que varíe levemente la performance cada vez que corremos Grid Search.\n",
    "\n",
    "Finalmente, en Randomized Search, con la posibilidad de que *max_depth* tome cualquier valor entre 1 y 30, confirmamos estas afirmacione: valores mayores a 10 tienden al overfitting y valores cercanos a 4 varían levemente su performance dependiendo del *splitter* utilizado.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gaussian Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = GridSearchCV(GaussianNB(), {}, cv=5)\n",
    "clf.fit(X_np, y_np)\n",
    "top_resultados(clf, 1, 'Gaussian')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Al igual que en LDA, no existen hiperparámetros que ajustar. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters_svm = {\n",
    "    'C': [1, 0.1, 0.01, 0.0001, 0.00001]\n",
    "}\n",
    "clf = GridSearchCV(LinearSVC(), parameters_svm, cv=5)\n",
    "clf.fit(X_np, y_np)\n",
    "top_resultados(clf, 10, 'Linear SVM - Grid Search')\n",
    "\n",
    "parameters_svm = {\n",
    "    'C': np.linspace(0.00001,0.01,100)\n",
    "}\n",
    "n_iter = 30\n",
    "clf = RandomizedSearchCV(LinearSVC(), parameters_svm, n_iter, cv=5)\n",
    "clf.fit(X_np, y_np)\n",
    "top_resultados(clf, n_iter, 'Linear SVM - Randomized Search')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Del total de 12 parámetros de la función del modelo solo *C* es hiperparámetro. Ignoramos el hiperparámetro *gamma*, por no estar en la función de *sklearn*, y *kerner*, ya que sólo trabajaremos con *lineal* por disposición de la consigna. \n",
    "\n",
    "El parametro *C* regula la proporción de clasificaciones incorrectas que se va a permitir a un determinado SVM. Valores de C más grandes resultarán en un hiperplano de margen más pequeño. \n",
    "\n",
    "Se varió C utilizando los valores 1, 0.1, 0.01, 0.0001, 0.00001 en Grid Search a lo largo de distintas iteraciones, observando que el *accuracy* sobre los datos de entrenamiento disminuyen al dismuirlo. Esta mejora tiene su límite al llegar a valores de C muy bajos (alrededor de 1e-5), donde comienza a empeorar la clasificación. Esto se debe a que el entrenamiento permite encontrar hiperplanos muy permisivos con valores mal clasificados.1.\n",
    "\n",
    "A continuación corrimos la búsqueda con Randomized Search para un rango continuo de valores de C entre 0.01 y 0.00001, tomando 100 valores. Observamos nuevamente un comportamiento similar, donde la performance aumenta a medida que disminuimos el C hasta un límite de 0.0003. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Conclusiones Grid Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Distintos sets de datos pueden ser mejor clasificados por unos u otros algoritmos. En este caso, las performances sobre los datos de validación siguen este orden decreciente: KNN, SVM, NB, DecisionTree, LDA.\n",
    "\n",
    "Tanto Naive Bayes como LDA cuentan con la desventaja de hacer suposiciones sobre la distribución de los datos, por lo que es posible que estas suposiciones no se cumplan y que sea ésta la causa de que no se encuentren entre los mejores modelos.\n",
    "\n",
    "Por otro lado, los otros tres son aquellos que no hacen suposiciones probabilísticas sobre los datos sino que se entrenan subdividiendo el espacio en el que se encuentran. Específicamente, aquellos que lo dividen de forma más compleja son los que generan mejor performance (SVM y KNN), mientras que los árboles de decisión, que dividen el espacio sólo con cortes rectos, fue el peor de los tres. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame()\n",
    "\n",
    "df[\"Model\"] = ['LDA', 'Decision Tree', 'Naive Bayes', 'SVM','KNN']\n",
    "df[\"mean_score_validation\"] = [0.68, 0.7175, 0.73, 0.7775, 0.7775]\n",
    "df[\"mean_score_training\"] = [0.9794, 0.8575, 0.7788, 0.9394, 0.7756]\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ejercicio 4: \n",
    "### Diagnóstico Sesgo-Varianza. \n",
    "\n",
    "En este punto, se pide inspeccionar dos de sus mejores modelos encontrados hasta ahora: el mejor modelo de tipo árbol de decisión y el mejor de tipo SVM. Para ello:\n",
    "\n",
    "1. Graficar curvas de complejidad para cada modelo, variando la profundidad en el caso de árboles, y el hiperparámetro C en el caso de SVM. Diagnosticar cómo afectan al sesgo y a la varianza esos dos hiperparámetros.\n",
    "2. Graficar curvas de aprendizaje para cada modelo. En base a estas curvas, sacar conclusiones sobre si los algoritmos parecen haber alcanzado su límite, o bien si aumentar la cantidad de datos debería ayudar.\n",
    "3. Construir un modelo RandomForest con 200 árboles. Explorar para qué sirve el hiperparámetro max_features y cómo afecta a la performance del algoritmo mediante una curva de complejidad. Explicar por qué creen que se dieron los resultados obtenidos. Por último, graficar una curva de aprendizaje sobre los parámetros elegidos para determinar si sería útil o no conseguir más datos (usar  grid search para encontrar una buena combinación de parámetros).  \n",
    "\n",
    "\n",
    "**Atención**: Tener en cuenta que debemos seguir utilizando ROC AUC como métrica para estas curvas.\n",
    "\n",
    "**ver**: http://scikit-learn.org/stable/modules/learning_curve.html#learning-curve\n",
    "\n",
    "----\n",
    "**EJERCICIO EXTRA:** Utilizar RandomizedSearchCV para explorar la performance del algoritmo de Gradient Boosting y comparar con los resultados obtenidos en el punto (c).\n",
    "\n",
    "\n",
    "----\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0,0.5,'Accuracy')"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "max_depths = list(range(1, 11)) + [50, 100, 150, None]\n",
    "auc_roc = []\n",
    "\n",
    "for max_depth in max_depths:\n",
    "    tree = DecisionTreeClassifier(max_depth=max_depth)\n",
    "    tree.fit(X_train_np, y_train_np)\n",
    "        \n",
    "    prediction_eval = tree.predict_proba(X_test_np)[:,1]\n",
    "    auc_roc.append(roc_auc_score(y_test_np, prediction_eval))\n",
    "\n",
    "\n",
    "fig = plt.plot(max_depths, auc_roc , marker=\"*\", color=\"blue\")\n",
    "plt.title(\"Curva de complejidad para un modelo de Decision Tree Classifier\")\n",
    "plt.xlabel(\"Profundidad del árbol\")\n",
    "plt.ylabel(\"ROC-AUC\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'SVC' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-5472705e0cf5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mC\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mCs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0msvm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSVC\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mC\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mC\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0msvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_np\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_np\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'SVC' is not defined"
     ]
    }
   ],
   "source": [
    "Cs = np.linspace(0.00001, 0.01, 50)\n",
    "auc_roc = []\n",
    "\n",
    "for C in Cs:\n",
    "    svm = LinearSVC(C=C)\n",
    "    svm.fit(X_train_np, y_train_np)\n",
    "        \n",
    "    prediction_eval = svm.predict_proba(X_test_np)[:,1]\n",
    "    auc_roc.append(roc_auc_score(y_test_np, prediction_eval))\n",
    "\n",
    "\n",
    "fig = plt.plot(Cs, auc_roc, marker=\"*\", color=\"green\")\n",
    "plt.title(\"Curva de complejidad para un modelo de Support Vector Machine\")\n",
    "plt.xlabel(\"C\")\n",
    "plt.ylabel(\"ROC-AUC\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los errores de predicción pueden ser clasificados en errores de sesgo, errores de varianza y errores irreductibles. Los dos primeros pueden ser mejorados. Para hacerlo, se buscará obtener un método de aprendizaje que logre disminuir al mismo tiempo sesgo y varianza. Se entiende por sesgo las suposiciones que los algoritmos hacen para poder estimar la función target que se desee aprender. Por varianza, la cantidad que varía la estimación al cambiar el set de datos de entrenamiento. Lo que suele ocurrir es que métodos más flexibles en cuanto a las suposiciones, es decir, con menor sesgo, resultan en una mayor varianza y, al revés. Se incurre, así, en un problema de trade-off entre sesgo y varianza. \n",
    "\n",
    "En el caso de los árboles de decisión, este trade-off depende de la profundidad del árbol, es decir, del parámetro max_depth. Un árbol de mayor profundidad hará muy pocas suposiciones sobre los datos y se adaptará muy bien a lo que le sea pasado como entrenamiento, aumentando así la varianza en gran medida al actuar sobre otro set de datos. \n",
    "En caso de los Support Vector Machines, el trade-off depende del parámetro C. Un aumento de este parámetro permite hacer más suposiciones sobre los datos achicando el margen de missclasification del hiperplano. Esto aumenta el sesgo y por consiguiente reduce la varianza. \n",
    "\n",
    "En los gráficos obtenidos. \n",
    "\n",
    "A partir de los gráficos realizados observamos que para el caso de árbol de decisión a medida que aumentamos la profundidad del árbol más allá de 3 nodos baja el *accuracy*. Esto es consistente con lo explicado anteriormente.\n",
    "En caso de SVM se puede ver que hay una tendencia creciente a medida que aumenta el C con estancamientos y máximos locales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from learning_curve import plot_learning_curve\n",
    "title = \"Learning Curves (SVM)\"\n",
    "estimator = LinearSVC(C=0.0001)\n",
    "plot_learning_curve(estimator, title, X_dev_np, y_dev_np, ylim=(0.7, 0.95), cv=5, n_jobs=4)\n",
    "\n",
    "title = \"Learning Curves (Decision Tree Classifier)\"\n",
    "estimator = DecisionTreeClassifier(max_depth=3, criterion='entropy', splitter='best')\n",
    "plot_learning_curve(estimator, title, X_dev_np, y_dev_np, (0.6, 1.05), cv=5, n_jobs=4)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La curva de aprendizaje muestra el score de validación y de entrenamiento de un estimador variando el número de las muestras de entrenamiento. Sirve para averiguar cuanto se beneficia el modelo de agregar nueva data de entrenamieno y si el estimador sufre más de error de sesgo o de varianza. \n",
    "Si ambos errores estuvieran presentes ambos scores convergerian a un valor demasiado bajo a medida que aumenta el tamaño del set de entrenamiento. En este caso, no se percibirán beneficios al aumentar los datos de entrenamiento. \n",
    "En cambio si el score de entrenamiento es más grande que el de validación para el máximo número de muestras de entrenamiento, agregar más muestras mejorará la generalización.  \n",
    "\n",
    "Nuestros gráficos se encuentran en el segundo caso, ya que la cantidad de datos que tenemos para entrenamiento parecen no ser suficientes para alcanzar la convergencia. Una posible solución sin necesidad de aumentar la cantidad de datos (muchas veces imposible, como en este caso) sería aumentar la cantidad de folds. De esta forma, tendriamos menos datos para la validacion pero más para el entrenamiento, logrando una mejor generalización de los modelos. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "max_features = [5, 50, 100, 200, 'sqrt', 'log2']\n",
    "auc_roc = []\n",
    "\n",
    "for max_feature in max_features :\n",
    "    random_forest = RandomForestClassifier(n_estimators=200, criterion='entropy', max_depth=3, max_features=max_feature)\n",
    "    auc_roc.append(np.mean(cross_val_score(random_forest, X_train_np, y_train_np, cv=5)))\n",
    "\n",
    "\n",
    "fig = plt.plot(range(len(max_features)), auc_roc, marker=\"*\", color=\"red\")\n",
    "plt.xticks(range(6), ['5', '50', '100', '200', 'sqrt', 'log2'])\n",
    "plt.title(\"Curva de complejidad para un modelo de Random Forest\")\n",
    "plt.xlabel(\"Cantidad máxima de features\")\n",
    "plt.ylabel(\"Accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El método Random Forest crea una serie de árboles de decisión. Su particularidad reside en que, al momento de hacerlo, cada vez que se considera un split, se elige como candidatos del split de manera random una muestra m de predictores de entre la totalidad del set p. Y de estos m elegidos al azar, el split solo puede elegir uno. Esto hace que una nueva muestra de predictores sea elegida cada vez. \n",
    "\n",
    "max_features determina el número de features considerado al momento de buscar el mejor split. Puede ocurrir: \n",
    "\n",
    "- Si es un int, entonces considera la cantidad max_features de features en cada split. \n",
    "- Si es float, entonces la cantidad max_features es una fracción fraction y se considera int(max_features * n_features) en cada split. \n",
    "- Si es sqrt, entonces max_features=sqrt(n_features)\n",
    "- Si es log2, entonces max_features=log2(n_features)\n",
    "- Si es None, entonces max_features=n_features\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notamos que Random Forest no tiene un comportamiento estable en cada ajuste realizado del modelo. Creemos que esto es debido a que tenemos un gran número de features (200) para la cantidad de datos que procesamos (320). Decidimos agregar Cross-Validation para poder quedarnos con un modelo más robusto. Incluso con esas condiciones, seguimos notando que la performance del modelo varía bastante en cada corrida."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters_rf = {\n",
    "    'criterion': ('gini', 'entropy'), \n",
    "    'max_features':[5, 50, 100, 200, 'auto', 'log2']\n",
    "}\n",
    "clf = GridSearchCV(RandomForestClassifier(n_estimators=200, max_depth=3), parameters_rf, cv=5)\n",
    "clf.fit(X_np, y_np)\n",
    "top_resultados(clf, 10, 'Random Forest Classifier - Grid Search')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title = \"Learning Curves (Random Forest)\"\n",
    "estimator = RandomForestClassifier(criterion='entropy', max_depth=3, max_features=50, n_estimators=200)\n",
    "plot_learning_curve(estimator, title, X_dev_np, y_dev_np, ylim=(0.7, 1.05), cv=5, n_jobs=4)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A partir de la curva de aprendizaje vemos que mejoraría el modelo si aumentamos la cantidad de datos para el proceso de entrenamiento."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Boosting Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters_gb = {\n",
    "    'n_estimators': range(10,50), \n",
    "    'max_features':[5, 50, 100, 200, 'auto', 'log2'], \n",
    "    'loss' : ('deviance', 'exponential')\n",
    "}\n",
    "n_iter = 8\n",
    "clf = RandomizedSearchCV(GradientBoostingClassifier(), parameters_gb, n_iter, cv=5)\n",
    "clf.fit(X_np, y_np)\n",
    "top_resultados(clf, n_iter, 'Gradient Boosting Classifier - Randomized Search')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notamos que para 200 arboles el modelo overfittea, por lo que en la segunda iteración decidimos agregar para que utilice menos cantidad de árboles.\n",
    "Podemos observar, que no hay grandes mejoras entre utilizar Random Forest o Gradient Boosting como algoritmo de clasificación. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Competencias\n",
    "\n",
    "La entrega del trabajo estará acompañada de una competencia en la cual deberán poner a prueba su mejor modelo y sobre todo, su capacidad para estimar sus resultados. \n",
    "\n",
    "Su tarea será estimar la performance (AUC ROC) que tendrá su mejor modelo en datos de evaluación (X_competencia). \n",
    "\n",
    "Para ello, deberán predecir las probabilidades de las distintas instancias con su modelo, enviarnos dichas probabilidades junto a una estimación con 4 decimales de cuál será el AUC ROC resultante y calcularemos el resultado real. El grupo que consiga acercarse más al valor real, será el grupo ganador.  \n",
    "\n",
    "Recomendamos no perder de vista esta competencia en el momento de separar los datos en los primeros puntos. \n",
    "\n",
    "Para esto, junto con la entrega del informe, deberán enviar un archivo en formato csv con las columnas “index” y “output” (ver ejemplo de archivo en: [y_competencia_ejemplo.csv](https://github.com/pbrusco/aa-notebooks/blob/master/TP1/y_competencia_ejemplo.csv)) y un valor esperado de AUC ROC. \n",
    "\n",
    "\n",
    "## Entrega\n",
    "- Contarán con un esqueleto en formato Jupyter Notebook en donde tendrán que completar las celdas faltantes (ya sea con explicaciones y gráficos o código). \n",
    "- El notebook final deberá ser entregado en formatos .html e .ipynb. Es necesario que los resultados puedan reproducirse al ejecutar todas las celdas en orden (Kernel - Restart and Run All) utilizando las bibliotecas requeridas en el archivo: requirements.txt del repositorio. \n",
    "- Tienen tiempo hasta las 23:59hs del día miércoles 17/10/2018. La entrega se debe realizar a través del campus virtual y debe contener el informe.\n",
    "- El trabajo deberá elaborarse en grupos de 3 personas.\n",
    "- Se podrán pedir pruebas de integridad y autoría; es decir, verificar que la salida solicitada es fruto del modelo presentado y que el modelo fue construido según lo requerido en este enunciado.\n",
    "- La evaluación será grupal y se basará en la calidad del informe (presentación, claridad, prolijidad); la originalidad, practicidad y coherencia técnica de la solución; la corrección y solidez de las pruebas realizadas.\n",
    "- En el primer parcial se incluirá una pregunta sobre la solución entregada. Esa pregunta no influirá en la nota del parcial, pero sí en la nota individual del TP1.\n",
    "- La participación en la competencia es obligatoria. De todas maneras, el resultado no incidirán en la nota de la materia.\n",
    "- Los ejercicios extra son opcionales para aprobar el TP, pero son obligatorios para promocionar la materia.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.69\n",
      "AUC - ROC 0.8245192307692308\n"
     ]
    }
   ],
   "source": [
    "clf = KNeighborsClassifier(n_neighbors = 67, algorithm='brute', weights= 'uniform')\n",
    "clf.fit(X_np, y_np)\n",
    "\n",
    "prediction_eval_c = clf.predict(X_eval)\n",
    "accuracy = accuracy_score(y_eval, prediction_eval_c)\n",
    "prediction_eval_p = clf.predict_proba(X_eval)[:,1]\n",
    "roc = roc_auc_score(y_eval, prediction_eval_p)\n",
    "\n",
    "print('accuracy', accuracy)\n",
    "print('AUC - ROC', roc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finalmente, luego de haber entrenado y testeado varios modelos, con varias combinaciones de hiperparámetros decidimos seleccionar KNN con los siguientes valores:\n",
    "\n",
    "- *n_neighbors = 67*\n",
    "- *algorithm   = brute*\n",
    "- *weights     = uniform*\n",
    "\n",
    "El accuracy de este modelo se encontraba alrededor de **0.77**, tanto para los datos de entrenamiento como para los datos de testeo utilizados durante el desarrollo. Sin embargo, para estimar el valor que se obtendrá al clasificar los datos de la competencia utilizados datos que no fueron utilizados en ninguna otra etapa: los datos held-out.\n",
    "\n",
    "Al entrenar el modelo con todos los datos de desarollo y testearlo con los datos held-out observamos un esperado descenso en el *accuracy*, obteniendo un valor de **0.69**. Por otro lado, el valor de AUC-ROC es de **0.6842**, valores que esperamos repetir con los datos predichos para la competencia.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_competencia = clf.predict(X_competencia)\n",
    "y_competencia = pd.DataFrame(data=prediction_competencia, index=X_competencia.index)\n",
    "y_competencia.to_csv('y_competencia.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
